{
  "um welcome everybody to lesson 5 and so",
  "we have officially peaked and everything",
  "is down here from here as of halfway",
  "through the last lesson we started with",
  "computer vision because it's the most",
  "mature kind of out-of-the-box ready to",
  "use deep learning application it's",
  "something which if you're not using deep",
  "learning you won't be getting good",
  "results so the difference you know",
  "hopefully between not during lesson one",
  "versus doing less than one you've gained",
  "a new capability you didn't have before",
  "and you kind of get to see a lot of the",
  "kind of tradecraft of training and",
  "effective neural net and so then we",
  "moved into NLP because text is kind of",
  "another one which you really kind of",
  "can't do really well without deep",
  "learning generally speaking and it's",
  "just got to the point where it's pretty",
  "you know works pretty well now in fact",
  "the New York Times just featured an",
  "article about the latest advances in",
  "deep learning for text yesterday and",
  "talked quite a lot about the work that",
  "we've done in that area along with open",
  "AI and Google and Allen Institute of",
  "artificial intelligence and then we've",
  "kind of finished our application journey",
  "with tabula and collaborative filtering",
  "partly because tabular and collaborative",
  "filtering of things that you can still",
  "do pretty well without deep learning so",
  "it's not such a big step it's not a kind",
  "of whole new thing that you could do",
  "that you couldn't used to do and also",
  "because the you know we're going to try",
  "to get to a point where we understand",
  "pretty much every line of code and the",
  "implementations of these things and the",
  "implementations of those things it's",
  "much less intricate than",
  "vision and NLP so as we come down this",
  "other side of the journey which is like",
  "all the stuff we've just done how does",
  "it actually work by by starting where we",
  "just ended which is starting with",
  "collaborative filtering and then tabular",
  "data",
  "we're going to be able to see what all",
  "those lines of code do by the end of",
  "today's lesson that's our goal so",
  "particularly this lesson you should not",
  "expect to come away knowing how to solve",
  "you know how to do applications you",
  "couldn't do before but instead you",
  "should have a better understanding of",
  "how we've actually been solving the",
  "applications we've seen so far",
  "particularly we're going to understand a",
  "lot more about regularization which is",
  "how we go about managing over versus",
  "under fitting and so hopefully you can",
  "use some of the tools from this lesson",
  "to go back to your previous projects and",
  "get a little bit more performance or",
  "handle models where previously maybe you",
  "felt like your data was not enough or",
  "maybe your underfitting",
  "and so forth and it's also going to lay",
  "the groundwork for understanding",
  "convolutional neural networks and",
  "recurrent neural networks that will do",
  "deep dives into in the next two lessons",
  "and as we do that we're also going to",
  "look at some new applications some new",
  "vision and NLP applications let's start",
  "where we left off last week do you",
  "remember this picture so this picture we",
  "were looking at kind of waters a deep",
  "neural net look like and we had various",
  "layers and the first thing we pointed",
  "out is that there are only and exactly",
  "two types of layer there are layers that",
  "contain parameters and there are layers",
  "that contain activations parameters the",
  "things that your model",
  "learns they're the things that you use",
  "gradient descent to go parameters -",
  "equals learning rate times parameters",
  "grad okay that's our basic that's what",
  "we do okay and those parameters are used",
  "by multiplying them by input activations",
  "doing a matrix product so the yellow",
  "things are our weight matrices your",
  "weight tensors more generally but that's",
  "plus enough so we take some input",
  "activations or some layer activations",
  "and we multiply it by weight matrix to",
  "get a bunch of activations so",
  "activations numbers that these are",
  "numbers that are calculated okay so I",
  "find in our study group I keep getting",
  "questions about where does that number",
  "come from and I always answer it in the",
  "same way you tell me is it a parameter",
  "or is it an activation because it's one",
  "of those two things okay that's where",
  "numbers come from",
  "I guess inputs a kind of a special",
  "activation so they're not calculated",
  "they're just there so maybe that's a",
  "special case so maybe it's an input or a",
  "parameter or an activation activations",
  "don't only come out of matrix",
  "multiplications",
  "they also come out of activation",
  "functions and the most important thing",
  "to remember about an activation function",
  "is that it's an element-wise function so",
  "it's a function that is applied to each",
  "element of the input that evasions in",
  "turn and creates one activation for each",
  "input element so if it starts with a",
  "twenty long vector it creates a twenty",
  "long vector by looking at each one of",
  "those in turn doing one thing to it and",
  "spitting out the answer okay so an",
  "element-wise function value is the main",
  "one we've looked at and honestly it",
  "doesn't too much matter which you pick",
  "so we don't spend much time talking",
  "about activation functions because if",
  "you just use RAL you you'll get a pretty",
  "good answer pretty much all the time and",
  "so then we learnt that this combination",
  "of matrix multiplications followed by",
  "values",
  "stack together has this amazing",
  "mathematical property called the",
  "universal approximation theorem which is",
  "if you have big enough weight matrices",
  "and enough of them it can solve any",
  "arbitrarily complex mathematical",
  "function to any arbitrarily high level",
  "of accuracy assuming that you can train",
  "the parameters both in terms of time and",
  "data availability and so forth okay so",
  "that's the bit which I find particularly",
  "more advanced computer scientists get",
  "really confused about is they're always",
  "asking like where's the next bit what's",
  "the trick how does it work but that's it",
  "you know you just do those things and",
  "you pass back the gradients and you",
  "update the weights with the learning",
  "rate and that's it so that piece where",
  "we take the loss function between the",
  "actual targets and the output of the",
  "final layer for the final activations we",
  "calculate the gradients with respect to",
  "all of these yellow things and then we",
  "update those yellow things by learning",
  "rate by subtracting learning rate times",
  "the gradient that process of calculating",
  "those gradients and then subtracting",
  "like that is called back propagation",
  "okay so when you hear the term well",
  "that's this very small fun so when you",
  "see when you hear the term back",
  "propagation it's one of these terms that",
  "neural networking folks love to use it",
  "sounds very impressive okay that you can",
  "replace it with your head with weights",
  "minus equals weight start grad times",
  "learning rate or parameters I should say",
  "rather than weights a bit more general",
  "okay",
  "so that's what we covered last week and",
  "then I mentioned last week that we're",
  "going to cover a couple more things I'm",
  "going to come back to these ones cross",
  "entropy and softmax later today let's",
  "talk about fine-tuning",
  "now so what happens when we take a res",
  "net 34 and we do transfer learning",
  "what's actually going on so the first",
  "thing to notice is there is net 34 that",
  "that we grabbed from image net has a",
  "very specific weight matrix at the end",
  "it's a weight matrix that has 1000",
  "columns why is that because image net",
  "the problem they asked you to solve in",
  "the image net competition is please",
  "figure out which one of these 1000 image",
  "categories this picture is so that's why",
  "they need a thousand things here because",
  "an image net this target vector is",
  "length a thousand it's a you've got to",
  "pick the probability that it's which one",
  "of those thousand things so there's a",
  "couple of reasons this weight matrix is",
  "no good to you when you're doing",
  "transfer learning the first is that you",
  "probably don't have a thousand",
  "categories you know I was trying to do",
  "teddy bears black bears or brown bears",
  "so I don't want a thousand categories so",
  "the second is even if I did have it",
  "exactly a thousand categories they're",
  "not the same thousand categories that",
  "are in image net so basically this whole",
  "weight matrix is a waste of time for me",
  "so what do we do we throw it away so",
  "when you go create CNN in first AI it",
  "deletes that and what does it do instead",
  "instead it puts in two new weight",
  "matrices in there for you with a rel you",
  "in between and so there are some",
  "defaults as to what size this first one",
  "is but the second one the size there is",
  "as big as you need it to be so in your",
  "data bunch which you passed your learner",
  "from that we know how many activations",
  "you need if you're doing classification",
  "it's wherever many classes you have if",
  "you're doing regression it so if in many",
  "numbers you're trying to predict in the",
  "regression problem and so remember that",
  "in your if your data bunch is called",
  "data that'll be called data dot C so",
  "we'll add for you",
  "the",
  "weight matrix of size dado C by however",
  "much was in the previous layer okay so",
  "now we need to train those because",
  "initially these weight matrices are full",
  "of random numbers okay because new",
  "weight matrices are always full of",
  "random numbers if they're new and these",
  "ones and you we're just we've grabbed",
  "them and thrown them in there so we need",
  "to train them but the other layers are",
  "not new the other layers are good at",
  "something right and what are they good",
  "at well let's remember that xyler and",
  "furgus paper here are examples of some",
  "visualization of some filters some some",
  "weight matrices in the first layer and",
  "some examples of some things that they",
  "found right so the first layer had one",
  "part of the weight matrix was good at",
  "finding diagonal edges in this direction",
  "and then in layer two one of the filters",
  "was good at finding corners in the top",
  "left and then in layer three one of the",
  "filters was good at finding repeating",
  "patterns another one was good at finding",
  "round orange things another one was good",
  "at finding kind of like fairy your",
  "floral textures so as we go up they're",
  "becoming more sophisticated but also",
  "more specific right so like layer four I",
  "think was finding like eyeballs for",
  "instance now if you're wanting to",
  "transfer and learn to something for",
  "histopathology slides there's probably",
  "going to be no eyeballs in that right so",
  "the later layers are no good for you but",
  "they'll certainly be some repeating",
  "patterns and they'll certainly be some",
  "diagonal edges right so the earlier you",
  "grow in the model the more likely it is",
  "that you want those weights to stay as",
  "they are well to start with we",
  "definitely need to train these new",
  "weights because they're random so let's",
  "not bother training any of the other",
  "weights at all to start with so what we",
  "do is we basically say let's freeze",
  "let's freeze all of those other layers",
  "so what does that mean all that means is",
  "that we're asking first di and PI torch",
  "that when we train you know however many",
  "epochs we do when we call fit don't back",
  "propagate the weights but don't prep a",
  "back propagate the gradients back into",
  "those layers in other words when you go",
  "parameters equals parameters - learning",
  "rate times gradient only do it for the",
  "new layers don't bother doing it for the",
  "other layers",
  "that's what freezing means okay just",
  "means don't update those parameters so",
  "it'll be a little bit faster as well",
  "because there's a few less calculations",
  "to do it'll take up a little bit less",
  "memory because there's a few less",
  "gradients that we have to store but most",
  "importantly it's not going to change",
  "weights that are already better than",
  "nothing they're better than random at",
  "the very least so that's what happens",
  "when you call freeze it doesn't freeze",
  "the whole thing it freezes everything",
  "except the randomly generated atom added",
  "layers that we put on for you so then",
  "what happens next okay after a while we",
  "say okay this is looking pretty good we",
  "probably should train the rest of the",
  "network now so we unfreeze okay and so",
  "now we're gonna chain the whole thing",
  "but we still have a pretty good sense",
  "that these new layers we added to the",
  "end probably need more training and",
  "these ones right at the start that might",
  "just be like diagonal edges probably",
  "don't need much training at all so we",
  "split our our model into a few sections",
  "right and we say let's give different",
  "parts of the model different learning",
  "rates so this part of the model we might",
  "give a learning rate of 1e neg 5 and",
  "this part of the model we might give a",
  "learning rate of 1",
  "a neg 3c and so what's gonna happen now",
  "is that we can keep training the entire",
  "network but because the learning rate",
  "for the early layers is smaller it's",
  "going to move them around less because",
  "we think they're already pretty good and",
  "also like if it's already pretty good",
  "too the optimal value if you used a",
  "higher learning rate it could kick it",
  "out right it could actually make it",
  "worse which we really don't want to",
  "happen okay so this this process is",
  "called using discriminative learning",
  "rates you won't find much online about",
  "it because I think we were kind of the",
  "first to use it for this purpose or at",
  "least talked about it extensively maybe",
  "other probably other people used it",
  "without writing it down so most of the",
  "stuff you'll find about this will be",
  "fast AI students but it's it's starting",
  "to get more well-known slowly now but",
  "it's a really really important concept",
  "for transfer learning without using this",
  "you just can't get nearly as good",
  "results so how do we do discriminative",
  "learning rates in fast AI when you when",
  "you anywhere you can put a learning rate",
  "in fast AI such as with the fit function",
  "the first thing you put in is the number",
  "of epochs and then the second thing you",
  "put in is learning rate saying if you",
  "use fit one cycle the learning rate you",
  "can put a number of things that you can",
  "put a single number like one a neg three",
  "you can write a slice so you can write",
  "slice for example one a neg three with a",
  "single number or you can write slice",
  "with two numbers",
  "which of those men in the first case",
  "just using a single number means every",
  "layer gets the same learning rate so",
  "you're not using discriminative learning",
  "rates if you pass a single number to",
  "slice it means the final layers get a",
  "learning rate of whatever you wrote down",
  "of whatever you wrote down one e neck 3",
  "and then all the other layers get the",
  "same learning rate which is that divided",
  "by 3 okay so all of the other layers",
  "will be one a neg 3/3 the last layers",
  "will be one in x3 in the last case the",
  "final layers the these randomly hadn't",
  "added layers will still be again 1 enoch",
  "3 the first layers will get 1 in egg 5",
  "and the other layers will get learning",
  "rates that are equally spread between",
  "those two so ink it multiplicatively",
  "equal right so if there were three",
  "layers there would be one in egg five",
  "one in egg for one in egg three so equal",
  "multiples each time one slight tweak to",
  "make things a little bit simpler to",
  "manage we don't actually give a",
  "different learning rate to every layer",
  "we give a different learning rate to",
  "every layer group which is just we",
  "decided to put the groups together for",
  "you and so specifically what we do is",
  "the randomly added extra layers we call",
  "those one layer group this is by default",
  "you can modify it and then all the rest",
  "we split in half into two layer groups",
  "so by default at least with a CNN you'll",
  "get three layer groups and so if you say",
  "slice one enoch in egg three you",
  "will get one in egg five learning rate",
  "for the first layer group one in egg",
  "four for the second one day neck three",
  "for the third so now if you go back and",
  "look at the way that we're training",
  "hopefully you'll see that this makes a",
  "lot of sense this divided by three thing",
  "there's a little weird and we won't talk",
  "about why that is until part two of the",
  "course it says",
  "specific quirk around batch",
  "normalization so we can discuss that in",
  "the advanced topic if anybody's",
  "interested all right",
  "so that is that is fine tuning so",
  "hopefully that makes that a little bit",
  "less mysterious so we were looking at",
  "collaborative filtering last week and in",
  "the collaborative filtering example we",
  "called fit one cycle and we passed in",
  "just a single number and that makes",
  "sense because in collaborative filtering",
  "we only have one layer there's a few",
  "different pieces in it but there isn't",
  "you know a matrix multiply followed by",
  "an activation function followed by",
  "another matrix multiply I'm going to",
  "introduce a another piece of jargon here",
  "they're not always exactly matrix",
  "multiplications there's something very",
  "similar there they're linear functions",
  "that we add together but the more",
  "general term for these for these things",
  "that are written more general than",
  "matrix multiplications is our fine",
  "functions okay so if you hear me say the",
  "word a fine function you can replace it",
  "in your head with matrix multiplication",
  "but as we'll see when we do convolutions",
  "convolutions our matrix multiplications",
  "where some of the weights are tied and",
  "so it would be slightly more accurate to",
  "call them f-fine functions and I'd like",
  "to introduce a little bit more drug in",
  "each lesson so that when you you know",
  "read books or papers or watch other",
  "courses or read documentation there will",
  "be more of the words you're recognized",
  "okay so when you see a fine function it",
  "just means a linear function right and",
  "it means something very very close to",
  "matrix multiplication now matrix",
  "multiplication is the most common kind",
  "of fi-in function at least in deep",
  "learning so specifically for",
  "collaborative filtering",
  "the model we were using was this one it",
  "was where we had a bunch of numbers here",
  "and a bunch of numbers here and we took",
  "the dot product of them and given that",
  "one here is a row and when there's a",
  "column we can actually that's the same",
  "as a matrix product so M mult in Excel",
  "multiplies matrices so here's the matrix",
  "product of those two and so I started",
  "this training last week by using solver",
  "in Excel and we never actually went back",
  "to see how it went so let's go and have",
  "a look now so the average sum of squared",
  "error got down to 0.39 so we're trying",
  "to predict something on a scale of 0.5",
  "to 5 so on average we're being wrong by",
  "about 0.4 that's pretty good and you can",
  "kind of see it's pretty good if you look",
  "at like 3 5 1 is what it meant to be",
  "three point two five five point one or",
  "0.98 that's pretty close right so you",
  "get the general idea and then I started",
  "to talk about this idea of embedding",
  "matrices and so in order to understand",
  "that let's put this worksheet aside now",
  "look at another worksheet so here's",
  "another worksheet and what I've done",
  "here is I have copied over those two",
  "weight matrices from the previous",
  "worksheet right here's the one for users",
  "and here's the one for movies and the",
  "movies one I've transposed it so it's",
  "now got exactly the same dimensions as",
  "the users one okay so the here are two",
  "weight matrices initially they were",
  "random we can train them with gradient",
  "descent in the original data the user",
  "IDs and movie IDs were numbers like",
  "these okay to make life more convenient",
  "I've converted them to numbers from 1 to",
  "15 okay so in these columns I've got for",
  "every rating",
  "I've got user ID",
  "movie ID rating using these mapped",
  "numbers so that they're contiguous",
  "starting at one okay now I'm going to",
  "replace user ID number one with this",
  "vector the vector contains a 1 followed",
  "by 14 zeros and then user number 2 I'm",
  "going to replace with a vector of 0 and",
  "then 1 and then 13 zeros and so forth so",
  "movie ID 14 or these are movie ID 14",
  "I've also replaced with another vector",
  "which is 13 zeros and then a 1 and then",
  "a 0 okay so these are called one-hot",
  "encodings by the way so this is not part",
  "of a neural net this is just like some",
  "input pre-processing room literally",
  "making this my new input so this is my",
  "new inputs for the mo movies this is my",
  "new inputs for my users ok so these are",
  "the inputs to a neural net so what I'm",
  "going to do now is I'm going to take",
  "this input matrix and I'm going to do a",
  "matrix multiplied by this weight matrix",
  "and that'll work",
  "because this has 15 rows and this has 15",
  "columns so I can multiply those two",
  "matrices together because they match and",
  "you can do matrix multiplication in",
  "Excel using the air mult function just",
  "be careful if you're using Excel because",
  "this is a function that returns multiple",
  "numbers you can't just hit enter when",
  "you finish with it you have to hit",
  "control shift enter control shift enter",
  "means this is a array function is",
  "something that returns multiple values",
  "so here is the matrix product of this",
  "input matrix of per of of inputs and",
  "this make this parameter matrix or",
  "weight matrix so that's just a normal",
  "neural network layer okay it's just",
  "a regular matrix model play and so we",
  "can do the same thing for movies and so",
  "here's the matrix model play for movies",
  "well here's the thing this input is we",
  "claim is this kind of one hot encoded",
  "version of user ID number one and these",
  "activations are the activations for user",
  "ID number one why is that because if you",
  "think about it",
  "the matrix multiplication between a one",
  "hot encoded vector and some matrix is",
  "actually going to find the nth row of",
  "that matrix when the one is in position",
  "in does that make sense so what we've",
  "done here is we've actually got a matrix",
  "multiply that is creating this these",
  "output activations right but it's doing",
  "it in a very interesting way which is",
  "it's effectively finding a particular",
  "row when the input matrix so having done",
  "that we can then multiply those two sets",
  "together just a dot product and we can",
  "then find the loss squared and then we",
  "can find the average loss and lo and",
  "behold that number 0.39 is the same as",
  "this number because they're doing the",
  "same thing so this one was kind of",
  "finding this particular users embedding",
  "vector this one is just doing a matrix",
  "multiply and therefore we know they are",
  "mathematically identical so let's lay",
  "that out again so here's our final",
  "version this is the same weight matrices",
  "again exactly the same I've copied them",
  "over and here's those user IDs and movie",
  "IDs again right but this time I've laid",
  "them out just in a normal kind of",
  "tabular form just like you would expect",
  "to see",
  "the input to your model and this time I",
  "have got exactly the same set of",
  "activations here that I had here but in",
  "this case I've calculated these",
  "activations using excels offset function",
  "which is an array look up right it says",
  "find the first row of this so this is",
  "doing it as an array lookup so this",
  "version is identical to this version but",
  "obviously it's much less memory",
  "intensive and much faster because I",
  "don't actually create the one hot",
  "encoded matrix and I don't actually do a",
  "matrix multiply because that matrix",
  "multiply is nearly all multiplying by",
  "zero which is a total waste of time so",
  "in other words multiplying by a one",
  "handed matrix is identical to doing an",
  "array lookup therefore we should always",
  "do the array lookup version and",
  "therefore we have a specific way of",
  "doing we have a specific way of saying I",
  "want to do a matrix multiplication by a",
  "one hot encoded matrix without ever",
  "actually creating it I'm just instead",
  "going to pass in a bunch of intz and",
  "pretend they're one not encoded and that",
  "is called",
  "and embedding right so you might have",
  "heard this word embedding all over the",
  "places if there's some magic advanced",
  "mathy thing but embedding means look",
  "something up in an array okay but it's",
  "interesting to know that looking",
  "something up in an array is",
  "mathematically identical to doing a",
  "matrix product by a one hot encoded",
  "matrix and therefore an embedding fits",
  "very nicely in our standard model of our",
  "neural networks work so now suddenly",
  "it's as if we have another whole kind of",
  "layer it's a kind of layer where we get",
  "to look things up in an array but we",
  "actually didn't do anything special",
  "right we just added this computational",
  "shortcut this thing called an embedding",
  "which is simply a fast memory efficient",
  "way of multiplying by",
  "hot encoded matrix okay so this is",
  "really important because when you hear",
  "people say embedding you need to replace",
  "it in your head with an array lookup",
  "which we know is mathematically",
  "identical to matrix multiplied by a one",
  "hot encoder matrix here's the thing",
  "though it has kind of interesting",
  "semantics right because when you do",
  "multiply something by a one hot encoded",
  "matrix you get this nice feature where",
  "the rows of your weight matrix the",
  "values only appear for row number one",
  "for example where you get user ID number",
  "one in your inputs right so in other",
  "words you kind of end up with this",
  "weight matrix where certain rows of",
  "weights correspond to certain values of",
  "your input and that's pretty interesting",
  "it's particularly interesting here",
  "because going back to a kind of most",
  "convenient way to look at this because",
  "the only way that we can calculate an",
  "output activation is by doing a dot",
  "product of these two input vectors that",
  "means that they kind of have to",
  "correspond with each other right like",
  "there has to be some way of saying if",
  "this number for a user is high and this",
  "number for a movie is high then the user",
  "will like the movie so the only way that",
  "can possibly make sense is if these",
  "numbers represent features of personal",
  "taste and corresponding features of",
  "movies for example the movie has John",
  "Travolta in it and user ID likes John",
  "Travolta then you'll like this movie",
  "okay so like we're not actually deciding",
  "the Rhodes mean anything we're not doing",
  "anything to make the Rhodes mean",
  "anything but the only way that this",
  "gradient descent could possibly come up",
  "with a good answer is if it figures",
  "out what the aspects of movie taste are",
  "and the corresponding features of movies",
  "are so those underlying kind of features",
  "that appear that are called latent",
  "factors or latent features they're these",
  "hidden things that were there all along",
  "and once we train this neural net they",
  "suddenly appear now here's the problem",
  "no one's going to like Battlefield Earth",
  "right it's not a good movie even though",
  "it has John Travolta in it so how are we",
  "going to deal with that right because",
  "there's this feature called",
  "I like John Travolta movies and this",
  "feature called this movie has John",
  "Travolta and so this is now like you're",
  "gonna like the movie but we need to save",
  "some way to say unless it's Battlefield",
  "Earth or you're a Scientologist either",
  "one right so how do we do that we need",
  "to add in bias right so here is the same",
  "thing again same weight matrix sorry not",
  "the same weight because he's the same",
  "construct right same shape of everything",
  "but this time you've got an extra row so",
  "now this is not just the matrix product",
  "of that and that but I'm also adding on",
  "this number and this number which means",
  "now each movie can have an overall this",
  "is a great movie versus this isn't a",
  "great movie and every user can have an",
  "overall this user rates movies highly or",
  "this user doesn't rate movies highly so",
  "that's called the bias so this is",
  "hopefully going to look very familiar",
  "right this is the same usual linear",
  "model concept or linear layer concept",
  "from a neural net that you have a matrix",
  "product and a bias and remember from",
  "lesson to lesson 2's genie SGD notebook",
  "you never actually need a bias you could",
  "always just add a column of ones to your",
  "input data and then that gives you bias",
  "for free but that's pretty inefficient",
  "right so in practice all neural networks",
  "library explicitly have a concept",
  "of bias we don't actually add the column",
  "of ones so what does that do well just",
  "before I came in today I ran tools",
  "solver or notes data solver on this as",
  "well and we can check the RMS see and so",
  "the root mean squared here is 0.32",
  "versus the version without bias whereas",
  "0.39 okay so you can see that this",
  "slightly better model gives us a better",
  "result and it's better because it's it's",
  "it's giving both more flexibility right",
  "and it's also just makes sense",
  "semantically that you need to be able to",
  "say it's not the weather I'd like the",
  "movie is not just about the combination",
  "of what actors it has and whether it's",
  "dialogue-driven and how much action is",
  "in it but just is it a good movie",
  "okay or am i somebody who rates movies",
  "highly okay so there's all the pieces of",
  "this collaborative filtering model how",
  "are we going sancisco any questions we",
  "have three questions okay okay so a",
  "first question then is when we load a",
  "pre trained model can we explore the",
  "activation grids to see what they might",
  "be good at recognizing yes you can and",
  "we will learn how to should be in the",
  "next lesson can we have an explanation",
  "of what the first argument in fit one",
  "cycle actually represents is it",
  "equivalent to an epoch yes",
  "the first argument to fit one cycle or",
  "fit is number of epochs it's in other",
  "words an epoch is looking at every input",
  "once so if you do ten epochs you're",
  "looking at every every input ten times",
  "and so there's a chance you might start",
  "overfitting if you've got lots of lots",
  "of parameters and a high learning rate",
  "if you only do one epoch it's impossible",
  "to have a fit",
  "and so that's why it's kind of useful to",
  "remember how many epochs you're doing",
  "can we have an exponent that one what is",
  "an affine function an affine function is",
  "a linear function I don't know if we",
  "need much more detail than that",
  "if you're multiplying things together",
  "and adding them up it's an air fine",
  "function I'm not going to bother with",
  "them",
  "exact mathematical definition partly cuz",
  "I'm a terrible mathematician and partly",
  "because it doesn't matter but if you",
  "just remember that you're multiplying",
  "things together and then adding them up",
  "that's the most important thing it's",
  "linear okay and therefore if you put an",
  "F fine function on top of an F fine",
  "function that's just another F fine",
  "function you haven't won anything at all",
  "that's a total waste of time right so",
  "you need to sandwich it with any kind of",
  "non-linearity pretty much works right",
  "including replacing the negatives with",
  "zeros which we call value okay so if you",
  "draw Fi and really F I'm really f I'm",
  "really you you have a deep neural",
  "network okay so so let's go back to the",
  "collaborative filtering notebook and",
  "this time we're going to grab the whole",
  "movie lens 100k dataset there's also a",
  "20 million dataset",
  "by the way so really a great project",
  "available made by this group called",
  "group lens they actually update the",
  "movie lens data sets on a regular basis",
  "but they helpfully provide the original",
  "one and we're going to use the original",
  "one because that means that we can",
  "compare two baselines because everybody",
  "basically they say hey if you're going",
  "to compare the baselines make sure you",
  "all use the same data set here's the one",
  "you should use unfortunately it means",
  "that we're going to be restricted to",
  "movies that are before 1998 so maybe you",
  "won't have seen them all but that's the",
  "price we pay you can replace this with",
  "ml latest when you download it and use",
  "it if you want to play around with",
  "movies that are up to date okay",
  "the original movie lens data set the",
  "more recent ones are in a CSV file it's",
  "super convenient to use the original one",
  "is a slightly messy first of all they",
  "don't use commas for two limiters they",
  "use tabs so in pandas you can just say",
  "what's the delimiter and you loaded in",
  "the second is they don't add a header",
  "row so that you know what color room is",
  "what so you have to tell pandas there's",
  "no header row and then since there's no",
  "header row you have to tell pandas what",
  "are the names of four columns rather",
  "than that that's what we need okay so we",
  "can then have a look at head which",
  "remembers the first few rows and there",
  "is our user ratings user movie rating",
  "and let's make it more fun let's see",
  "what the movies actually are I'll just",
  "point something out here which is",
  "there's this thing called encoding",
  "equals I'm going to get rid of it and I",
  "get this error Unicode I just want to",
  "point this out because you'll all see",
  "this at some point in your lives codec",
  "can't decode blah blah blah what this",
  "means is that this is not a Unicode file",
  "this will be quite common when you're",
  "using datasets are a little bit older",
  "back before you know us folks in the",
  "West really realized that there are",
  "people that use languages other than",
  "well English people we English languages",
  "other than English nowadays you know",
  "we're much better at handling different",
  "languages we use this standard called",
  "Unicode for it and Python very helpfully",
  "uses Unicode by default but so if you",
  "try to load an old file it's not Unicode",
  "you actually believe it or not have to",
  "guess how it was coded but since like",
  "it's really likely that it was created",
  "by you know some Western European or",
  "American person they almost certainly",
  "used Latin one so if you just papi an",
  "encoding equals Latin one if you use",
  "file open in Python or pandas open or",
  "whatever that will generally get around",
  "your problem again they didn't have the",
  "names so we had to list of the names are",
  "this is kind of interesting they had a",
  "separate column for every one of however",
  "many genres they had 19 genres and",
  "you'll see it this looks one hot encoded",
  "but it's actually not it's actually n",
  "hot encoded in other words a movie can",
  "be in multiple genres we're not going to",
  "look at Jean Roos today but it's just",
  "interesting to point out that this is a",
  "way that sometimes people will represent",
  "something like genre and the more recent",
  "version they actually listed the genres",
  "directly which is much more convenient",
  "ok so I find life is so we got a hundred",
  "thousand ratings I find life is easier",
  "when you're modeling when you actually",
  "denormalize the data so I actually want",
  "the movie title directly in my ratings",
  "so pandas has a merge function to let us",
  "do that so here's the ratings table with",
  "actual titles so as per usual we can",
  "create a data bunch for our application",
  "so a collab data bunch for the collab",
  "application from what from a data frame",
  "there's our data frame set aside some",
  "validation data really we should use the",
  "validation sets and cross validation",
  "approach that they used if you're going",
  "to properly compare with a benchmark so",
  "take these comparisons with a grain of",
  "salt",
  "by default car lab data bunch assumes",
  "that your first column is you user",
  "second column of item the third column",
  "is rating but now we're actually going",
  "to use the title column as item so we",
  "have to tell it what the item column",
  "name is and then all of our data bunches",
  "support show batch so you can just check",
  "what's in there and there it is okay so",
  "I'm going to try and get as good a",
  "result as I can so I'm gonna try and use",
  "whatever tricks I can come up with to",
  "get a good answer now one of the tricks",
  "is to use the Y range and remember the",
  "the Y range was the thing that made the",
  "final activation function a sigmoid and",
  "specifically last week we said let's",
  "have a sigmoid that goes from naught to",
  "5 and that way it's going to ensure that",
  "it kind of is going to help the neural",
  "network predict things that are in the",
  "range actually didn't do that in my",
  "Excel version and so you can see I've",
  "actually got some negatives Maddon",
  "there's also some things bigger than",
  "five so if you want to beat me in Excel",
  "you could you could add the sigmoid to",
  "excel and train this and you'll get a",
  "slightly better answer now the problem",
  "is that a sigmoid actually asymptotes at",
  "say whatever the maximum is we said five",
  "which means you can never actually",
  "predict five but plenty of movies have a",
  "rating of five so that's a problem so",
  "actually it's slightly better to make",
  "your way range go from a little bit less",
  "than the minimum to a little bit more",
  "than the maximum and the minimum of this",
  "data is 0.5 and the maximum is 5 so this",
  "range is just a little bit further so",
  "that's a that's one little trick to get",
  "a little bit more accuracy the other",
  "trick I used is to add something called",
  "weight decay and we're going to look at",
  "that next ok after this section we got",
  "to learn about weight okay so then how",
  "many how many factors do you want or",
  "what are factors the number of factors",
  "is the width of the embedding matrix so",
  "why don't we say embedding sighs maybe",
  "we should but in the world of",
  "collaborative filtering they don't use",
  "that word they use the word factors",
  "because of this idea of latent factors",
  "and because the standard way of doing",
  "collaborative filtering has been with",
  "something called matrix factorization",
  "and in fact what we just saw happens to",
  "actually be a way of doing matrix",
  "factorization so we've we've actually",
  "accidentally learned how to do matrix",
  "factorization today so so this is a term",
  "that's kind of specific to this domain",
  "okay but you can just remember it as the",
  "width of the embedding matrix and so why",
  "40 well this is one of these",
  "architectural decisions you have to play",
  "around with and see what works so I",
  "tried 10 20 40 and 80 and I found 42 in",
  "to work pretty well and it rained really",
  "quickly so like you can check it in a",
  "little for loop",
  "to try a few things and see what looks",
  "best and then for learning rates so",
  "here's the learning rate finder as usual",
  "so five Enoch three seemed to work",
  "pretty well remember this is just a rule",
  "of thumb right five e neg three is a bit",
  "lower than both Silver's rule in my rule",
  "so Silver's role is find the bottom and",
  "go back by ten",
  "so his rule would be more like two e neg",
  "- I reckon my rule is kind of find about",
  "the steepest section which is about here",
  "which again like often it agrees with",
  "your man so that would be about - Enoch",
  "- I tried that but I always like to try",
  "like 10 X less than 10x more just to",
  "check and actually I found a bit less",
  "was helpful so the answer to the",
  "question like should I do blah is always",
  "try blah and see now that's how you",
  "actually become a good practitioner so",
  "that gave me point eight one three right",
  "and as usual you can save the result to",
  "save you another 33 seconds from having",
  "to do it again later and so there's a",
  "library called Lib wreck and they",
  "published some benchmarks for movie lens",
  "100k and there's a root mean squared",
  "error section and about point nine one",
  "is about as good as they seem to have",
  "been able to get point nine one is the",
  "root mean square error we use the mean",
  "square error not the root so we have to",
  "go to point nine one squared which is",
  "0.8 three and what we're getting point",
  "eight one so that's cool with this very",
  "simple model we're doing a little bit",
  "better quite a lot better actually",
  "although as I said take it with a grain",
  "of salt because we're not doing the same",
  "spits and the same cross validation that",
  "so we're at least highly competitive",
  "with their approaches okay so we're",
  "going to look at the Python code that",
  "does this in a moment we're going to",
  "look at the Python code that does this",
  "in a moment but for now just take my",
  "word for it that we're going to say",
  "something that's just doing this",
  "right looking things up in an array and",
  "then model plugging them together adding",
  "them up and doing the mean square error",
  "loss function so given that and given",
  "that we noticed that the only way that",
  "that can do anything interesting is by",
  "trying to kind of find these latent",
  "factors it makes sense to look and see",
  "what they found",
  "right particularly since as well as",
  "finding latent factors we also now have",
  "a specific bias number for every user",
  "and every movie right now you could just",
  "say what's the average rating to each",
  "movie but there's a few issues with that",
  "in particular this is something you see",
  "a lot with like anime people who like",
  "anime just love anime right and so",
  "they're watching lots of anime and then",
  "they just rate all the enemy highly and",
  "so very often on kind of charts of",
  "movies you'll see a lot of anime at the",
  "top particularly if it's like you know a",
  "hundred long series of anime you'll find",
  "you know every single item of that",
  "series in the top thousand movie lists",
  "or something so how do we deal with that",
  "well the nice thing is that instead if",
  "we look at the movie bias right the",
  "movie bias says kind of once we've",
  "included the user bias right which for",
  "an anime lover might be a very high",
  "number because they're just reading a",
  "lot of movies highly now once we account",
  "for the specifics of this kind of movie",
  "which again might be people love anime",
  "right what's left over is something",
  "specific to that movie itself so it's",
  "kind of interesting to look at movie",
  "bias numbers as a way of saying what are",
  "the best movies or what people what do",
  "people really like as movies even if",
  "those people don't rate movies very",
  "highly or even if there does that movie",
  "doesn't have the kind of features that",
  "people tend to have rate rate highly so",
  "it's kind of nice",
  "it's funny to say this and I'm by a by",
  "using the bias we get an unbiased kind",
  "of movie score so how do we do that well",
  "to make it interesting because",
  "particularly because this data set only",
  "start only goes to 1998 let's only look",
  "at movies that are plenty of people",
  "watch right so we'll use pandas to grab",
  "our reading movie table grip it by title",
  "and then count the number of ratings and",
  "not measuring how high their rating just",
  "how many ratings do they have okay and",
  "so the top thousand is that is them",
  "other movies that have been rated the",
  "most and so there hopefully movies that",
  "we might have seen okay that's the only",
  "reason I'm doing this and so I've called",
  "this top movies by which I mean not not",
  "good movies just movies were likely to",
  "have seen so not surprisingly Star Wars",
  "is the one that at that point most the",
  "most people were had put a rating to",
  "Independence Day there you go so we can",
  "then take our loner that we trained and",
  "asked it for the bias of the items",
  "listed here okay so is item equals true",
  "you would pass true to say I want the",
  "items or false to say I want the users",
  "and so this is kind of like a pretty",
  "common piece of nomenclature for",
  "collaborative filtering these IDs tend",
  "to be called users these IDs tend to be",
  "called items even if your problem has",
  "got nothing to do with users and items",
  "at all you know you just use these names",
  "for convenience okay so they're just",
  "they're just words",
  "so in our case we want the items this is",
  "the list of items we want we want the",
  "bias so this is specific to klepto",
  "filtering and so that's going to give us",
  "back a thousand numbers back because we",
  "asked for this has a thousand movies in",
  "it so we can now take and just for",
  "comparison let's also group the titles",
  "by the mean rating so then we can zip",
  "through so going through together",
  "each of the movies along with the bias",
  "and grab their rating and the bias and",
  "the movie and then we can sort them all",
  "by the zero index thing which is the",
  "bias so here are the lowest numbers so I",
  "can say you know",
  "Mortal Kombat annihilation not a great",
  "movie lawnmower man - not a great movie",
  "I haven't seen children of the corn but",
  "we did have a long discussion at SF",
  "study group today and people who have",
  "seen it agree not a great movie and you",
  "can kind of see like some of them",
  "actually have pretty decent ratings even",
  "as though like relative to right so this",
  "one's actually got a much higher rating",
  "than the next one right but you know",
  "that's kind of saying well the kind of",
  "actors that were in this in the kind of",
  "movie that this was and the kind of",
  "people who do like it and watch it you",
  "would expect it to be higher and then",
  "here's the sort by reverse okay",
  "Schindler's List Titanic Shawshank",
  "Redemption seems reasonable and again",
  "you can kind of look for ones where like",
  "the rating you know isn't that high but",
  "it's still very high here so that's kind",
  "of like you know at least in 1998 people",
  "weren't that into Leonardo DiCaprio or",
  "you know people aren't that into",
  "dialogue-driven movies or people aren't",
  "that into romances or whatever but still",
  "people liked it more than you would have",
  "expected so it's interesting to kind of",
  "like interpret our models in this way we",
  "can go a bit further and grab not just",
  "the biases but the weights so that is",
  "these things and again we're going to",
  "grab the weights for the items for our",
  "top movies and that is a thousand by",
  "forty because we asked for forty factors",
  "so rather than having a width of five we",
  "have a width of 40 often",
  "really there's there isn't really",
  "conceptually forty latent factors",
  "involved in taste and so trying to look",
  "at the forty can be you know not that",
  "intuitive so what we want to do is we",
  "want to squish those forty down to just",
  "three and there's something that we're",
  "not going to look into called PCA stands",
  "for principal components analysis so",
  "this is a movie W is a torch tensor and",
  "fast AI adds the PCA method to torch",
  "tensors and what PCA does principal",
  "components analysis is it's a simple",
  "linear transformation that takes an",
  "input matrix and tries to find a smaller",
  "number of columns that kind of cover a",
  "lot of the space of that original matrix",
  "if that sounds interesting which it",
  "totally is you should check out our",
  "course computational linear algebra",
  "which Rachele teachers where we will",
  "show you how to calculate PCA from",
  "scratch and why you'd want to do it and",
  "lots of stuff like that it's absolutely",
  "not a prerequisite for anything in this",
  "course but it's definitely worth knowing",
  "that taking layers of neural nets and",
  "chucking them through PCA is very often",
  "a good idea because very often you have",
  "like way more activations than you want",
  "in a layer and there's all kinds of",
  "reasons you would might want to play",
  "with it for example Francisco who's",
  "sitting next to me today is has been",
  "working on something to do image",
  "similarity right and very weak",
  "similarity a nice way to do that is to",
  "compare activations from a model but",
  "often those activations will be huge and",
  "therefore your thing could be really",
  "slow and unwieldy so people often for",
  "something like image similarity will",
  "chuck it through a PCA first and that's",
  "kind of cool",
  "in our case we're just going to do it so",
  "that we take our 40 components down to",
  "three components so hopefully they'll be",
  "easier for us to interpret so we can",
  "grab each of those three factors will",
  "call them factor naught one and two",
  "and let's grab that movie components and",
  "then sort and now the thing is we have",
  "no idea what this is going to mean but",
  "we're pretty sure it's going to be some",
  "aspect of taste and movie feature so if",
  "we print it out the top and the bottom",
  "we can see that the highest ranked",
  "things on this feature you would kind of",
  "describe them as you know connoisseur",
  "movies I guess you know like Chinatown",
  "you know really classic Jack Nicholson",
  "movie",
  "everybody knows Casablanca and even like",
  "wrong trousers is like this kind of",
  "classic claymation movie",
  "and so forth right so yeah this this is",
  "definitely measuring like things that",
  "are very high on the kind of connoisseur",
  "level where else maybe home alone 3 not",
  "such a favorite with Connor says perhaps",
  "it's just not to say that there aren't",
  "people who don't like it right but",
  "probably not the same kind of people",
  "that would appreciate secrets and lies",
  "ok so you can kind of see this idea that",
  "this has found some feature of movies",
  "and a corresponding feature of the kind",
  "of things people like so let's look at",
  "another feature",
  "so here's factor number one so this",
  "seems to have found like okay these are",
  "just big hits that you could watch with",
  "the family you know these are definitely",
  "not that you know Trainspotting very",
  "gritty kind of you know thing so again",
  "it's kind of found this interesting",
  "feature of taste and we could even like",
  "draw them on a graph right I've just",
  "cuddled them randomly to make them",
  "easier to see and you can kind of see",
  "like and this is just the top 50 most",
  "popular movies by rating by how many",
  "times they've been rated and so kind of",
  "on this one factor you've got the head",
  "of the terminators really high up here",
  "and the kind of English Patient and",
  "students list at the other end and then",
  "kind of is your godfather and Auntie",
  "Python over here and Independence Day",
  "and lie a liar over there so you get the",
  "idea",
  "so that's kind",
  "it would be interesting to see if you",
  "can come up with some stuff at work or",
  "other kind of datasets where you could",
  "try to pull out some some features and",
  "play with them so how does that work",
  "any questions what okay the question is",
  "why am I sometimes getting negative loss",
  "when training you shouldn't be so you're",
  "doing something wrong so ask on show us",
  "your your penny particularly since",
  "people are uploading this I guess other",
  "people seen it too so to put it on the",
  "forum I mean they sit there doing",
  "negative love likelihood yeah so we're",
  "going to be learning about cross or",
  "entropy and negative log likelihood",
  "after the break",
  "today they are lost functions that have",
  "very specific expectations about what",
  "your input looks like and if your input",
  "doesn't look like that then they're",
  "going to give very weird answers so",
  "probably you press the wrong buttons so",
  "don't do that okay okay",
  "so we said collab learner and so here is",
  "the collab learner function the collab",
  "learner function as per usual takes a",
  "data bunch and normally learners also",
  "take something where you ask for",
  "particular architectural details in this",
  "case there's only one thing which does",
  "that which is basically do you want to",
  "use a multi-layer neural net or do you",
  "want to use a classic collaborative",
  "filtering and we're only going to look",
  "at the classic collaborative filtering",
  "today or maybe your briefly look at the",
  "other one too let's see and so what",
  "actually happens here well basically",
  "we're going to create we create a",
  "an embedding dot bias model and then we",
  "pass back a learner which has our data",
  "and that model so obviously all the",
  "interesting stuff is happening here and",
  "embedding drop bias so let's take a look",
  "at that",
  "I clearly press the wrong button",
  "embedding dot bias there we go okay so",
  "here's our embedding dot bias model it",
  "is a NN module so in in pi torch to",
  "remind you all PI torch layers and",
  "models are NN modules they are things",
  "that once you create them look exactly",
  "like a function you call them with",
  "parentheses and you pass them arguments",
  "but they're not functions they don't",
  "even have normally in Python to make",
  "something look like a function you have",
  "to give it a method called dunder call",
  "remember that means underscore",
  "underscore call underscore underscore",
  "which doesn't exist here and the reason",
  "is that pi torch actually expects you to",
  "have something called forward and that's",
  "what pi torch will call for you",
  "when you call it like a function so when",
  "this model is being trained to get the",
  "predictions it's actually going to call",
  "forward for us so this is where we do",
  "the calculations right to calculate our",
  "predictions so this is where you can see",
  "we grab our why is this users rather",
  "than user that's because everything's",
  "done a mini-batch at a time right so it",
  "is kind of when I read the forward in in",
  "a PI torch module I tend to ignore in my",
  "head the fact that there's a mini batch",
  "and I pretend there's just one because",
  "PI torch automatically handles all of",
  "the stuff about doing it to everything",
  "in the mini batch for you right so let's",
  "pretend there's just one user right so",
  "grab that user and what is this self dot",
  "u underscore weight self dot u",
  "underscore weight is",
  "bedding we create an embedding for each",
  "of users by factors items by factors",
  "users by one items by one well that",
  "makes sense right so what users by one",
  "is yeah that's the users bias right and",
  "then users by Factor is here so users by",
  "factors is the first couple so that's",
  "going to go in you underscore weight and",
  "users comma one is the third so that's",
  "going to go in you underscore bias so",
  "remember when PI torch creates our NN",
  "module",
  "it calls dunder init and so this is",
  "where we have to create our weight",
  "matrices right and we don't normally",
  "create the actual weight matrix tensors",
  "we normally use PI torches convenience",
  "functions to do that for us",
  "and we're going to see some of that",
  "after the break so for now just",
  "recognize that this function is going to",
  "create an embedding matrix for us it's",
  "going to be a PI torch and n dot module",
  "as well so therefore to actually pass",
  "stuff into that embedding matrix and get",
  "activations out you treat it as if it",
  "was a function okay stick it in",
  "parentheses so if you want to look in",
  "the PI that pipe torch source code and",
  "find n n dot embedding you will find",
  "there's something called dot forward in",
  "there which will do this array lookup",
  "for us so here's where we grab the users",
  "here's where we grab the items and so",
  "we've now got the embeddings for each",
  "right and so at this point we're kind of",
  "like here and we found that and that so",
  "we multiply them together and sum them",
  "up and then we add on the user bias and",
  "the item bias and then if we've got a",
  "wide range then we do our sigmoid",
  "trick and so the nice thing is you know",
  "and you now understand the entirety of",
  "this model and this is not just any",
  "model this is a model that we just found",
  "is at the very least highly competitive",
  "with and perhaps slightly better than",
  "some published table of pretty good",
  "numbers from a software group that does",
  "nothing about this so you're doing well",
  "right this is nice",
  "so that's probably a good place to have",
  "a break and so after the break we're",
  "going to come back and we're going to",
  "talk about the one piece of this puzzle",
  "we haven't learnt yet which is what the",
  "hell is this - okay so let's come back",
  "at 750 okay",
  "so this idea of interpreting embeddings",
  "is really interesting and as we'll see",
  "later in this lesson the the things that",
  "we create for categorical variables more",
  "generally in tabular data sets are also",
  "embedding matrices and again that's just",
  "a normal matrix multiplied by a one hot",
  "encoded input where we skip the",
  "computational computational and memory",
  "burden of it by doing it in a more",
  "efficient way and it happens to end up",
  "with these interesting semantics kind of",
  "accidentally and there was this really",
  "interesting paper by these folks who",
  "came second in a capital competition for",
  "something called AI Rossman will",
  "probably look in more detail at the",
  "rustman competition in part two I think",
  "we're gonna run out of time in part one",
  "but it's basically there's pretty",
  "standard tabular stuff the main",
  "interesting stuffs in the pre-processing",
  "and it was interesting because Eve they",
  "came second despite the fact that the",
  "person who came first and pretty much",
  "everybody else was the top of the",
  "leaderboard did a massive amount of",
  "highly specific feature engineering",
  "where else these folks did",
  "we're less feature engineering than",
  "anybody else but instead they used a",
  "neural net and this was at a time in",
  "2016 when just no one did that no one",
  "was doing neural nets for tabular data",
  "so they have you know the kind of stuff",
  "that we've been talking about kind of a",
  "rose there or at least was kind of",
  "popularized there and when I say",
  "popularized I mean only popularized a",
  "tiny bit it's still most people unaware",
  "of this idea but it's pretty cool",
  "because in their paper they showed that",
  "the main average percentage error for",
  "various techniques K nearest neighbors",
  "random forests and gradient boosted",
  "trees",
  "well first you know neural Nets just",
  "works work worked a lot better but then",
  "with entity embeddings which is what",
  "they call this just using entity",
  "matrices in capital data you can",
  "actually they actually added the entity",
  "embeddings to all of these different",
  "tasks after training them and they all",
  "got way better right so neural nets with",
  "entity embeddings are still the best but",
  "a random forest with empty embeddings",
  "was not at all far behind and you know",
  "that's often kind of that's kind of nice",
  "right because you could train these",
  "entity matrices for products or stores",
  "or genome motifs or whatever and then",
  "use them in lots of different models",
  "possibly you know using faster things",
  "like random forests about getting a lot",
  "of the benefits that was something",
  "interesting they took a two-dimensional",
  "projection of their of their embedding",
  "matrix for state for example German",
  "state because this was a German",
  "supermarket chain I think using the same",
  "kind of approach we did I don't remember",
  "if they use PCA or something else",
  "slightly different and then here's the",
  "interesting thing I've circled here you",
  "know a few things in this embedding",
  "space and I've circled it with the same",
  "color over here and here I've circled",
  "some same color over here and it's like",
  "oh my god the embedding projection has",
  "actually discovered geography",
  "like but they didn't do that right but",
  "it's it's it's found things that are",
  "near by each other in grocery purchasing",
  "patterns because this was about",
  "predicting how many sales there will be",
  "you know it's it there is some",
  "Geographic element of that in fact here",
  "is a graph of the distance between two",
  "embedding vectors so you can just take",
  "an embedding vector and say what's the",
  "sum of squared you know compared to some",
  "other embedding vector that's the",
  "Euclidean distance what's the distance",
  "in embedding space and then plotted",
  "against the distance in real life",
  "between shops and you get this very",
  "strong positive correlation here is an",
  "embedding space for the days of the week",
  "and as you can see there's a very clear",
  "path through them here's the embedding",
  "space for the month of the year and",
  "again there's a very clear path through",
  "them so like embeddings are amazing and",
  "I don't feel like anybody's even close",
  "to exploring the kind of interpretation",
  "that you could get right so if you've",
  "got genome motifs or plant species or",
  "products that your shop sells or",
  "whatever like it would be really",
  "interesting to train a few models and",
  "try and kind of fine tune some",
  "embeddings and then like start looking",
  "at them in these ways in terms of",
  "similarity to other ones and clustering",
  "them and projecting them into 2d spaces",
  "and whatever I think is really",
  "interesting now so we're trying to make",
  "sure we understood what every line of",
  "code did in this some pretty good collab",
  "liner model we built and so the one",
  "piece missing is this WD piece and WD",
  "start stands for weight decay so what is",
  "weight decay weight decay is a type of",
  "regularization what is regularization",
  "well let's start by going back to this",
  "nice little chart that Andrew owned did",
  "in his terrific machine learning course",
  "where he plot you know plotted some data",
  "and then",
  "showed a few different lines through it",
  "this one here because Andrews at",
  "Stanford he has to use Greek letters",
  "okay",
  "so we couldn't say this is a plus BX but",
  "you know if you want to go there theta",
  "naught plus theta 1 X here's a line",
  "right it's a line even if it's a Greek",
  "letters is still alone so here's a",
  "second-degree polynomial a plus BX plus",
  "CX squared bit of curve right and here's",
  "a high degree polynomial which is curvy",
  "as anything so models with more",
  "parameters tend to look more like this",
  "and so in traditional statistics we say",
  "hey let's use less parameters because we",
  "don't want it to look like this because",
  "if it looks like this then the",
  "predictions over here and over here",
  "they're going to be you're wrong right",
  "it's not going to generalize well we're",
  "overfitting",
  "so we avoid overfitting by using less",
  "parameters and so if any of you are",
  "unlucky enough to have been brainwashed",
  "by a background in statistics or",
  "psychology or econometrics or any of",
  "these kinds of courses you'll have you",
  "know you're gonna have to unlearn the",
  "idea that you need less parameters",
  "because what you instead need to realize",
  "this is you will fit this lie that you",
  "need less parameters because it's a",
  "convenient fiction for the real truth",
  "which is you don't want your function to",
  "be too complex and having less",
  "parameters is one way of making it less",
  "complex but what if you had a thousand",
  "parameters and 999 of those parameters",
  "were 1 a neg 9 well what if there was 0",
  "if there's 0 they're not they're not",
  "really there or if they want a neg 9",
  "they're hardly there right so like why",
  "can't I have lots of parameters if like",
  "lots of them are really small man the",
  "answer is you can okay you know so this",
  "this thing of like counting the number",
  "of parameters is how we limit complexity",
  "is actually",
  "extremely limiting it's a fiction that",
  "really has a lot of problems right and",
  "so if in your head",
  "complexity is scored by how many",
  "parameters you have you're doing it all",
  "wrong right score it properly right so",
  "why do we care why would I want to use",
  "more parameters because more parameters",
  "means more nonlinearities more",
  "interactions more curvy bits right and",
  "real life is full of curvy bits but real",
  "life does not look like this but we",
  "don't want them to be more curvy than",
  "necessary or more interacting than",
  "necessary so therefore let's use lots of",
  "parameters and then penalize complexity",
  "okay so one way to penalize complexity",
  "is as I kind of suggested before is",
  "let's sum up the value of your",
  "parameters now that doesn't quite work",
  "because some parameters are positive and",
  "some are negative right so what if we",
  "sum up the square of the parameters all",
  "right and that's actually a really good",
  "idea okay let's actually create a model",
  "and in the loss function we're going to",
  "add the sum of the square of the",
  "parameters now here's a problem with",
  "that though maybe that number is way too",
  "big and it's so big that the best loss",
  "is to set all of the parameters to zero",
  "now that would be no good",
  "right so actually we want to make sure",
  "that doesn't happen so therefore let's",
  "not just add the sum of the squares of",
  "the parameters to the model but let's",
  "multiply that by some number that we",
  "choose and that number that we choose in",
  "first AI is called WD okay so that's",
  "what",
  "we're gonna take our loss function and",
  "we're going to add to it the sum of the",
  "squares of parameters multiplied by some",
  "number WD what should that number be",
  "well generally it should be zero point",
  "one people with fancy machine learning",
  "PhDs are extremely skeptical and",
  "dismissive of any claims that a learning",
  "rate can be 3 in X 3 most of the time or",
  "a weight decay can be point 1 what's the",
  "time but here's the thing we've done a",
  "lot of experiments on a lot of data sets",
  "and we've had a lot of trouble finding",
  "anywhere a weight decay of 0.1 isn't",
  "great however we don't make that the",
  "default we actually make the default",
  "0.01 why because in those rare occasions",
  "where you have too much weight decay no",
  "matter how much you train it just never",
  "quite fits well enough where else if you",
  "have too little weight decay you can",
  "still train well you're just at to",
  "overfit so you just have to stop a",
  "little bit early so we've been a little",
  "bit conservative with our defaults but",
  "my suggestion to you is this now that",
  "you know that every learner has a wd",
  "argument and I should mention you won't",
  "always see it in this list right because",
  "there's this concept of kW args in",
  "Python which is basically parameters",
  "that are going to get passed up the",
  "chain to the next thing that we call now",
  "so basically all of the learners will",
  "call eventually this constructor and",
  "this constructor has a WD right so this",
  "is just one of those things that you can",
  "either look in the docs or you now know",
  "it anytime you're constructing a learner",
  "from pretty much any kind of function in",
  "fast AI you can pass WD okay and so",
  "passing 0.1 instead of the default point",
  "0 1 will often help ok so give it a go",
  "so what's really going on here it would",
  "be helpful I think to go back to lesson",
  "two SGD because everything we're doing",
  "the rest of today really is based on",
  "this right and this is where we created",
  "some data and then we try and then we",
  "add at a loss function MSE and then we",
  "created a function called update which",
  "calculated our predictions",
  "that's our weight make matrix multiply",
  "now this is just a one layer so there's",
  "no value we calculated our loss using",
  "that mean squared error we calculated",
  "the gradients using loss type backward",
  "we then subtracted in place the learning",
  "rate times the gradients and that is",
  "gradient descent so if you haven't",
  "reviewed lesson two SGD please do",
  "because this is where we're this is our",
  "starting point so if you don't get this",
  "then none of this is going to make sense",
  "if you watching the video maybe pause",
  "now go back re-watch this part of listen",
  "to make sure you get it remember a dot",
  "sub underscore is basically the same as",
  "a minus equals because a dotsub is",
  "subtract and everything in pi torch if",
  "you add an underscore to it means do it",
  "in place so this is updating our a",
  "parameters which started out as minus",
  "0.1 one we just barbeque pick those",
  "numbers and it gradually makes them",
  "better all right so let's write that",
  "down",
  "so we are trying to calculate the",
  "parameters I'm going to call them",
  "weights because this is just more common",
  "in kind of epoch tea or time tea and",
  "they're going to be equal to whatever",
  "the weights were in the previous epoch",
  "- our learning rate multiplied by it's",
  "the derivative of our loss function with",
  "respect to our weights at time t minus 1",
  "okay so that's that's what this is doing",
  "okay and we don't have to calculate the",
  "derivative because it's boring and",
  "because it computers do it for us fast",
  "and then they store it here for us so",
  "we're good to go okay so make sure",
  "you're exceptionally comfortable with",
  "either that equation or that line of",
  "code because they are the same thing",
  "where do we go from here all right so",
  "what's that what's our loss our loss is",
  "some function of our independent",
  "variable variables X and now weights",
  "right and in our case we're using mean",
  "squared error for example and it's",
  "between our predictions and our actuals",
  "right so where does X and W come in",
  "well our predictions come from running",
  "some model we'll call it m on those",
  "predictions and that model contains some",
  "weights all right so that's that's what",
  "our loss function might be and this",
  "might be you all kinds of other loss",
  "functions will see some more today and",
  "so that's what ends up creating",
  "grab over here so we're going to do",
  "something else we're going to add weight",
  "decay some number which in our case is",
  "0.1 x times the sum of weights squared",
  "okay so let's do that and let's make it",
  "interesting by not using synthetic data",
  "but let's do some real data and we're",
  "going to use em NIST the hand-drawn",
  "digits right but we're going to do this",
  "as a standard fully connected net not as",
  "a convolutional net because we haven't",
  "learnt the details of how to really",
  "create one of those from scratch so in",
  "this case is actually deep learning net",
  "provides amnesty as a python pickle file",
  "in other words it's a file that pickle",
  "that Python can just open up and it'll",
  "give you numpy arrays straight away and",
  "they're flat and umpire rays we don't",
  "have to do anything to them so go grab",
  "that and it's a gzip file so you can",
  "actually just gzip don't open it",
  "directly and then you can pick all",
  "download it directly and again encoding",
  "equals latin-1 because yeah you know and",
  "then we can just put that static that'll",
  "give us the training the validation and",
  "the test set I don't care about the test",
  "set so generally in Python if there's",
  "like something you don't care about you",
  "tend to use this special variable called",
  "underscore there's no reason you have to",
  "it's just kind of people know you mean I",
  "don't care about this right so there's",
  "our trading trading X&Y; and a valid X&Y;",
  "now this actually comes in as a as you",
  "can see if I print the shape 50,000 rows",
  "by 784 columns but those 784 columns are",
  "actually 28 by 28 pixel pictures so if I",
  "reshape one of them into a 28 by 28",
  "pixel picture and plot it",
  "right then you can see it's the number",
  "five okay so that's our data we've seen",
  "em nest before in its kind of pre",
  "reshaped version here it is in its",
  "flattened version so I'm going to be",
  "using it in its flattened version okay",
  "and currently they are numpy arrays I",
  "need them to be tensors so I can just",
  "map Torche tensor across all of them and",
  "so now they're tensors okay I may as",
  "well create a variable with the number",
  "of things I have which we normally call",
  "n and remember we normally have a thing",
  "called you know we don't use seed I mean",
  "the number of activations we need we",
  "actually say this is not going to be",
  "activation sorry this is going to be",
  "number of columns that's not a great",
  "name for it sorry okay so there we are",
  "and then the the why not surprisingly",
  "the minimum value is zero and the",
  "maximum value is nine because that's the",
  "actual number we're gonna predict great",
  "so in lesson two SGD we like we created",
  "a data where we actually added a column",
  "of ones on so that we didn't have to",
  "worry about bias we're not going to do",
  "that we're going to have plate watch to",
  "do that kind of implicitly for us we had",
  "to write our own MSC function we're not",
  "going to do that we had to write our own",
  "little matrix model location thing we're",
  "not going to do that we're gonna have",
  "plate or do all this stuff for us now",
  "okay",
  "and what's more and really important",
  "we're going to do mini batches right",
  "because this is a big enough data set we",
  "probably don't want to do it all at once",
  "so if you want to do mini batches so we",
  "could we're not going to use too much",
  "faster i stuff here apply torch has",
  "something called intense a data set that",
  "basically grabs a any kind of tensor",
  "sorry two tensors and creates a data set",
  "remember a data set is something where",
  "if you index into it you get back an x",
  "value and a y value just one of them",
  "okay so it kind of looks like it looks a",
  "lot like a list of XY tuples once you",
  "have a data set then you can use a",
  "little bit of convenience by calling",
  "data by",
  "create and what's that going to do is",
  "it's going to create data loaders for",
  "you a data loader is something which you",
  "don't say I want the first thing or the",
  "fifth thing you just say I want the next",
  "thing and it will give you a batch a",
  "mini batch of whatever size you asked",
  "for and specifically it'll give you the",
  "X and the y of a mini batch so if I just",
  "grab the next of the iterator this is",
  "just standard Python if you haven't used",
  "it a radius in Python before here's my",
  "training data loader that data bunch",
  "courier creates for you and you can",
  "check that as you would expect the X is",
  "64 by 784 because there's 784 pixels",
  "flattened out 64 in a mini batch and the",
  "Y is just 64 numbers there are things",
  "we're trying to predict so and you know",
  "if you look at the source code for data",
  "batch job create you'll see there's not",
  "much there but so feel free to do so we",
  "just make sure that like your training",
  "set gets shuffled randomly shuffled for",
  "you we make sure that the data is put on",
  "the GPU for you just a couple of little",
  "convenience things like that but don't",
  "let it be magic",
  "if it feels magic check out the source",
  "code to make sure you see what's going",
  "on okay",
  "so rather than do this y hat equals x at",
  "a thing we're going to create an NN",
  "module all right if you want to create",
  "an end up module that does something",
  "different to what's already out there",
  "you have to subclass it right so sub",
  "classing is very very very normal in",
  "plaid torch so if you're not comfortable",
  "with sub classing stuff in python go",
  "read a couple of tutorials to make sure",
  "you our main thing is you have to",
  "override the constructor dunder init and",
  "make sure that you call the super",
  "classes constructor because n n dot",
  "modules super classes constructor is",
  "going to like set it all up to be a",
  "proper n n dot module for you so if you",
  "try to using if you're trying to create",
  "your own PI torch subclass and things",
  "don't work it's almost certainly because",
  "you forgot this line of code alright so",
  "the only thing we want to add is we want",
  "to create",
  "and an attribute in our class which",
  "contains a linear layer and n n dot",
  "linear module what is an N n dot linear",
  "module it's something which does that",
  "but actually it doesn't only do that it",
  "actually is X at a plus B so in other",
  "words we don't have to add the column of",
  "ones okay that's all it does okay so if",
  "you want to play around why don't you",
  "try and create your own and end on",
  "linear class you could create something",
  "called my linear and it'll take you you",
  "know depending on your piped watch",
  "background an hour or two and then",
  "you'll feel like okay this is we don't",
  "want any of this to be magic and you",
  "know all of the things necessary to",
  "create this know so you know these are",
  "the kind of things that you should be",
  "doing for your assignments this week is",
  "not so much new applications but try to",
  "start writing more of these things from",
  "scratch and get them to work learn how",
  "to debug them check what's going in and",
  "out and so forth okay but we could just",
  "use n n dot linear and that's this going",
  "to do so it's going to have a def",
  "forward inner there goes a at X plus B",
  "right and so then in our forward how do",
  "we calculate the result of this well",
  "remember every NN dot module looks like",
  "a function so we pass our X mini-batch",
  "so I don't use xB to mean a batch of X",
  "to self dot Lin and that's going to give",
  "us back the result of the ax plus B on",
  "this mini batch so this is a logistic",
  "regression model a logistic regression",
  "model is also known as a neural net with",
  "no hidden layers so it's a one layer",
  "neural net no nonlinearities because",
  "we're doing stuff ourself a little bit",
  "we have to put the weight matrices the",
  "parameters onto the GPU manually so just",
  "type CUDA to do that so here's our model",
  "and as you can see the end module",
  "machinery has automatically given us a",
  "representation of it it's automatically",
  "stored the dot Lin thing and it's",
  "telling us what's inside it so there's a",
  "lot of little conveniences that PI torch",
  "does for us",
  "so if you look at now at model n you can",
  "see not surprisingly here it is perhaps",
  "the most interesting thing to point out",
  "is that our model automatically gets a",
  "bunch of methods and properties and",
  "perhaps the most interesting one is the",
  "one called parameters which contains all",
  "of the yellow squares from our picture",
  "but it contains our parameters it",
  "contains our weight matrices and biased",
  "matrices in as much as they're different",
  "so if we have a look at P dot shape for",
  "P and modeled up parameters there's",
  "something of ten by two 784 and there's",
  "something of ten so what are they well",
  "ten by 784 okay so that's the thing",
  "that's going to take in 784 dimensional",
  "input and spit out a 10 dimensional",
  "output because that's handy because our",
  "input is 784 dimensional and we need",
  "something that's going to give us a",
  "probability of ten numbers after that",
  "happens we've got ten activations which",
  "we then want to add the bias to so there",
  "we go here's a vector of length 10 so",
  "you can see why this this model we've",
  "created has exactly the stuff that we",
  "need to do our ax plus B so let's grab a",
  "learning rate we're going to come back",
  "to this loss function in a moment but we",
  "can't use em as well hmm we can't really",
  "use MSE for this right because we're not",
  "trying to see how close are you did you",
  "predict three and actually it was four",
  "gosh you were really close it's like no",
  "three is just as far away from four as",
  "zero is away from four when you're",
  "trying to predict what number did",
  "somebody draw so we're not going to use",
  "MSE we're going to use cross-entropy",
  "loss which we'll look at in a moment and",
  "here's our update function I copied it",
  "from less than two SGD but now we're",
  "calling our model rather than going a at",
  "X we're calling our model as if it was a",
  "function to get Y hat and we're calling",
  "our loss func rather than calling MSE to",
  "get our loss and then this is all the",
  "same as before except rather than going",
  "through each parameter and going",
  "parameter",
  "sub underscore learning rate times",
  "gradient we loop through the parameters",
  "okay because very nicely for us pipe",
  "torch will automatically create this",
  "list of the parameters of anything that",
  "we created in our dunder init and look",
  "I've added something else I've got this",
  "thing called w2 I go through HP and",
  "model drop parameters and I add two",
  "double to w2 the sum of squares so w-2",
  "now contains my summer squared sweets",
  "and then I multiply it by some number",
  "which I set to one a neg five so now I",
  "just implemented weight decay okay so",
  "when people talk about weight decay it's",
  "not an amazing magic complex thing",
  "containing thousands of lines of CUDA",
  "C++ code it's those two lines of Python",
  "that's weight okay this is not a",
  "simplified version that's just enough",
  "for now this is weight okay that's it",
  "okay and so here's the thing there's a",
  "really interesting kind of drool way of",
  "thinking about weight decay one is that",
  "we're adding the sum of squares weights",
  "and that seems like a very sound thing",
  "to do and it is and well let's go ahead",
  "and run this",
  "so here I've just got a list",
  "comprehension that's going through my",
  "data loader so the data loader gives you",
  "back one mini batch and it's for the",
  "whole thing giving you XY each time I'm",
  "gonna call update for each each one",
  "returns loss now PI torch tensors since",
  "I did it all on the GPU that's sitting",
  "in the GPU and it's like got all these",
  "stuff attached to it to calculate",
  "gradients it's going to use up a lot of",
  "memory so if you if you called dot item",
  "on a scalar tensor it turns it into an",
  "actual normal Python number so this is",
  "just means I'm returning back normal",
  "Python numbers and then I can plot them",
  "and yeah there you go my loss function",
  "is going down",
  "and you know it's really nice to try",
  "this stuff to see it behaves as you",
  "expect like we thought this is what",
  "would happen as we get closer and closer",
  "to the answer it bounces around more and",
  "more right because we're kind of close",
  "to where we should be",
  "it's kind of fitting flat probably",
  "flatter and weight space so we kind of",
  "jumping further and so you can see why",
  "we would probably want to be reducing",
  "our learning rate as we go learning rate",
  "annealing okay now here's the thing that",
  "is only interesting for training a",
  "neural net because it appears here",
  "because we take the gradient of it",
  "that's the thing that actually updates",
  "the weights right so they actually the",
  "only thing interesting about WD times",
  "sum of W squared is its gradient so we",
  "don't do a lot of math here but I think",
  "we can handle that the gradient of this",
  "whole thing if you remember back to your",
  "high school math is equal to the",
  "gradient of H part taken separately and",
  "then add them together so let's just",
  "take the gradient of that right because",
  "we already know the gradient of this is",
  "just whatever we had before right so",
  "what's the gradient of W D times the sum",
  "of W squared right let's remove the",
  "psalm and pretend there's just one",
  "parameter it doesn't change the",
  "generality of it so the gradient of W D",
  "times W squared so what's the gradient",
  "of that with respect to W it's just two",
  "WD times W ok and so remember this is",
  "our constant which now case was like",
  "well in that little loop it was 1 e neg",
  "5",
  "and that's our weights and like we could",
  "replace WD with like 2wd without loss of",
  "generality so let's throw away the two",
  "so in other words all weight decay does",
  "is it subtracts some constant times the",
  "weights every time we do a batch so",
  "that's why it's called weight decay okay",
  "when it's in this form where we add the",
  "square to the loss function that's",
  "called l2 regularization when it's in",
  "this form where we subtract WD times",
  "weights from the gradients",
  "that's called weight decay and they are",
  "kind of mathematically identical for",
  "everything we've seen so far in fact",
  "they are mathematically identical and",
  "we'll see in a moment a place where",
  "they're not where are things get",
  "interesting ok so this is just a really",
  "important tool you now have in your",
  "toolbox you can make giant neural",
  "networks right and still avoid",
  "overfitting by adding more weight decay",
  "okay or you could use really small data",
  "sets with moderately large sized models",
  "and avoid overfitting with weight decay",
  "it's not magic right like you might",
  "still find you don't have enough data in",
  "which case like you get to the point",
  "where you're not overfitting by adding",
  "lots of weight decay and it's just not",
  "training very well that can happen all",
  "right but at least this is something",
  "that Union can now play around with just",
  "to kind of go on here now that we've got",
  "this update function we could replace",
  "this M missed logistic with amnesty row",
  "Network and build a neural network from",
  "scratch right now we just need two",
  "linear layers right in the first one we",
  "could use a weight matrix of size 50 and",
  "so we didn't need to make sure that the",
  "second linear layer has an input of size",
  "50 so it matches the final layer has to",
  "have an output of size 10",
  "that's the number of classes we're",
  "predicting and so now our forward just",
  "goes to a linear layer calculate value",
  "to a second linear layer and now we've",
  "actually created a neural net from",
  "scratch I mean we didn't write it in",
  "linear but you can write it yourself or",
  "you could like do the matrices directly",
  "you know how to so again you know if we",
  "go model dot CUDA and then we can",
  "calculate losses for the exact same",
  "update function there it goes right so",
  "this is why this kind of idea of neural",
  "nets is so easy right once you have",
  "something that can do gradient descent",
  "right then you can try different models",
  "and then you can start to add more Pytor",
  "stuff so like rather than add doing all",
  "this stuff yourself why not just go opt",
  "equals opt e m dot something so there's",
  "something we've done so far is SGD and",
  "so now you're saying 2pi torch i want",
  "you to take these parameters and",
  "optimize them using SGD and so this now",
  "rather than saying for P in parameters P",
  "minus equals L R times P dot grad you",
  "just say up dot step it's the same thing",
  "okay it's just less code right but and",
  "it does the same thing but the reason",
  "it's kind of particularly interesting is",
  "that now you can replace SGD with atom",
  "for example and you can even add things",
  "like weight decay right because like",
  "there's more stuff it's kind of in these",
  "things for you right so that's why we",
  "tend to use you know optiom glass so",
  "behind the scenes this is actually what",
  "we do in first ago so if I go up to m",
  "dot sgt okay so this",
  "right and so that's that's just that",
  "picture but if we change to a different",
  "optimizer so look what happened it",
  "diverged and we've seen a great picture",
  "of that from one of our students who",
  "showed what divergence looks like this",
  "is what it looks like when you try to",
  "train something so let's use we're using",
  "a different optimizer so we need a",
  "different learning rate and you can't",
  "just continue training because by the",
  "time it's diverged the the the weights",
  "are like really really big and really",
  "really small they're not going to come",
  "back so start again okay there's a",
  "better learning rate but look at this",
  "we're down underneath point five by",
  "about epoch 200 where else before and",
  "I've even sure we ever got to quite that",
  "level so what's going on what's what's",
  "Adam let me show you and we're gonna do",
  "gradient descent in Excel because why",
  "wouldn't you okay so here is some",
  "randomly generated data okay some X's",
  "and some whites well they're actually",
  "they're randomly generated XS and the",
  "Y's are all calculated by doing ax plus",
  "B where a is 2 and B is 30 okay so this",
  "is some data that we got to try and",
  "match and here is SGD and so we got to",
  "do it with SGD now in our lesson to SGD",
  "notebook we did the whole data set at",
  "once as a batch in the notebook we just",
  "looked at we did mini batches in this",
  "spreadsheet we're going to do online",
  "gradient descent which means every",
  "single row of data is a batch there's",
  "kind of a batch size of one okay so as",
  "per usual we're going to start by",
  "picking an intercept and slope kind of",
  "arbitrarily so I'm just going to pick",
  "them at 1 doesn't really matter",
  "so here I've copied over the data this",
  "is my X&Y; and so my intercept and slope",
  "as I said is 1 all right I'm just",
  "literally referring back to this cell",
  "here so my prediction for this",
  "particular intercept them",
  "would be 14 times one plus one which is",
  "15 and so there's my error means that",
  "there's my summer Squared's but not even",
  "a sum at this point it's the squared",
  "error okay so now I need to calculate",
  "the gradient so that I can update",
  "there's two ways you can calculate the",
  "gradient one is analytically and so I",
  "you know you can just look them up on",
  "Wolfram Alpha or whatever so there's the",
  "gradients if you write it out by hand or",
  "look it up or you can do something",
  "called finite differencing because",
  "remember gradients just how far you move",
  "in act sorry how far you how far the the",
  "outcome moves divided by how far your",
  "change was for really small changes so",
  "let's just make a really small change so",
  "here we've taken our intercept and added",
  "point O 1 to it right and then",
  "calculated our our loss and you can see",
  "that our loss went down a little bit",
  "right and we added 0.01 here so our",
  "derivative is that difference divided by",
  "that point I 1 okay now that's called",
  "finite differencing and you can always",
  "do derivatives of find out different",
  "seeing it's slow we don't do it in",
  "practice but it's nice for just checking",
  "stuff out so we can do the same thing",
  "for our a term at 0.012 that take the",
  "difference and divide by 0.01 or as I",
  "say we can calculate it directly using",
  "the actual derivative analytical and you",
  "can see that you know that and that as",
  "you'd expect it's very similar and that",
  "and that not very similar so gradient",
  "descent then just says let's take our",
  "current value of that weight and",
  "subtract the learning rate times the",
  "derivative there it is okay and so now",
  "we can copy that intercept and that",
  "slope to the next row and do it again",
  "and do it lots of times and at the end",
  "we've done one epoch so at the end of",
  "that epoch",
  "we could say oh great so this is our",
  "slope so let's copy that over to where",
  "it says slope and this is our intercept",
  "so I'll copy it to where it says",
  "intercept and now it's done another",
  "epoch okay so that's kind of boring I'm",
  "copying and pasting so I created a very",
  "sophisticated macro which copies and",
  "pastes for you and so I just recorded it",
  "basically and so and then I created a",
  "very sophisticated for loop that goes",
  "through and does it five times and I",
  "attach that to the Run button so if I",
  "press run it'll go ahead and do it five",
  "times and just keep track of the era",
  "each time okay so that is SGD and as you",
  "can see it is just infuriatingly slow",
  "like particularly the intercept is meant",
  "sorry yeah it's meant to be 30 and we're",
  "still only up to one point five seven",
  "and like just it's just going so slowly",
  "so let's beat it up so the first thing",
  "we can do to speed it up is to use them",
  "in court momentum right so here's the",
  "exact same spreadsheet is the last",
  "worksheet I've removed the finite",
  "difference seeing version of the",
  "derivatives because they're not that",
  "useful does the analytical ones here and",
  "here's the thing where I take the the",
  "derivative and I'm going to update by",
  "the derivative but what I do it's kind",
  "of more interesting to look at this one",
  "is I take the derivative and I multiply",
  "it by 0.1 now what I do is I look at the",
  "previous update and I multiply that by",
  "0.9 and I add the two together so in",
  "other words the update that I do is not",
  "just based on the derivative but 1/10 of",
  "it is the derivative and 90% of it is",
  "just the same direction I went last",
  "and this is called momentum right what",
  "it means is remember how we kind of",
  "thought about what might happen",
  "if you're trying to find the minimum of",
  "this and you were here and your learning",
  "rate was too small right and you just",
  "keep doing the same steps or if you keep",
  "doing the same steps then if you also",
  "add in the step you talked last time and",
  "your steps are going to get bigger and",
  "bigger aren't they okay until eventually",
  "they go too far but now of course your",
  "gradients point in the other direction",
  "to whether your momentum is pointing so",
  "you might just take a little step over",
  "here and then you'll start going small",
  "steps bigger steps bigger steps small",
  "steps bigger stairs like that right so",
  "that's kind of what momentum does or if",
  "you're if you're kind of going too far",
  "like this which is also slow all right",
  "then the average of your last few steps",
  "is actually somewhere kind of between",
  "the two isn't it all right so this is a",
  "really common idea right it's like when",
  "you have something that says kind of my",
  "what is in this case it's like my step",
  "my step at time T equals some number",
  "people often use alpha because like I",
  "say they've got to love these Greek",
  "letters some number times the actual",
  "thing I want to do right so it might in",
  "this case it's like the gradient right",
  "plus one minus alpha times whatever you",
  "had last time st minus one this thing",
  "here",
  "is called an exponentially weighted",
  "moving average and the reason why is",
  "that if you think about it these 1 minus",
  "alphas are going to mount a play so s T",
  "minus 2 is in here with a kind of a 1",
  "minus alpha squared and s T minus 3 is",
  "in there with a net 1 minus alpha cubed",
  "so in other words this ends up being the",
  "actual thing I want plus a weighted",
  "average of the last few time periods",
  "where the most recent ones are",
  "exponentially higher weighted ok and",
  "this is going to keep popping up again",
  "and again all right so that's what",
  "momentum is it says I want to go based",
  "on the current gradient plus the",
  "exponentially weighted moving average of",
  "my last few steps so that's useful",
  "that's called SGD with momentum and we",
  "can do it by changing this here to",
  "saying SGD momentum and momentum 0.9 is",
  "really common it's couple add a lot it's",
  "like it's so common it's always pointing",
  "in just about four basic stuff so that's",
  "how you do rest you deal with momentum",
  "and and again it's not I didn't show you",
  "some simplified version I showed you the",
  "version that is that is SGD ok that's",
  "that's you again you can write your own",
  "try it out that would be a great",
  "assignment would be to take lesson to",
  "SGD and add momentum to it or even the",
  "new notebook we've got feminists get rid",
  "of the OP team dot and write your own",
  "update function with with momentum then",
  "there's a cool thing called rmsprop one",
  "of the really cool things about rmsprop",
  "is that Geoffrey Hinton created it",
  "famous neural net guy everybody uses it",
  "it's like really popular it's really",
  "common the correct citation for rmsprop",
  "is the Coursera online free MOOC that",
  "that's where he first mentioned rmsprop",
  "so I love this thing that like you know",
  "call new things appear in MOOCs that not",
  "a paper so rmsprop is very similar to",
  "momentum",
  "but this time we have an exponentially",
  "weighted moving average not of the",
  "gradient updates but of f/8 squared",
  "that's the gradient squared so what the",
  "gradient squared times 0.1 plus the",
  "previous value times 0.9 so it's",
  "exponentially this is an exponentially",
  "weighted moving average of the gradient",
  "squared so what's this number gonna mean",
  "well if my gradients really small and",
  "consistently really small this will be a",
  "small number if my gradient is highly",
  "volatile it's going to be a big number",
  "or if it's just really big all the time",
  "it'll be a big number and why is that",
  "interesting because when we do a update",
  "this time we say wait - learning rate",
  "times gradient divided by the square",
  "root of this so in other words if our",
  "gradients consistently very small and",
  "not volatile let's take bigger jumps and",
  "that's kind of what we want right when",
  "we watched how the intercept moves so",
  "damn slowly but it just it's like",
  "obviously you need to just try it go",
  "faster so if I now run this after just",
  "five epochs this is already up to three",
  "right where else with the basic version",
  "after five epochs it's still at 1.27 and",
  "remember we have to get to 30 so the",
  "obvious thing to do and by obvious I",
  "mean only a couple of years ago did",
  "anybody actually figure this out is do",
  "both right so that's called Adam so Adam",
  "is simply keep track of the",
  "exponentially weighted moving average of",
  "the gradient squared and also keep track",
  "of the exponentially weighted moving",
  "average of my steps right and both",
  "divided by the exponentially weighted",
  "moving average of the squared terms",
  "and you know take point nine of a step",
  "in the same direction as last time so",
  "it's it's momentum and rmsprop that's",
  "court Adam and look at this okay",
  "five steps we're at 25 okay so you know",
  "these these are these optimizes people",
  "call them dynamic learning rates a lot",
  "of people have the misunderstanding that",
  "you don't have to set a learning rate of",
  "course you do right it's just like",
  "trying to identify parameters that need",
  "to move faster you know or consistently",
  "go in the same direction it doesn't mean",
  "you don't need learning rates we still",
  "have a learning rate okay and in fact",
  "you know if I run this again but",
  "currently my my error not distribute so",
  "it we trying to get to 30 comma 2 so if",
  "I run it again it's getting better but",
  "eventually now it's just moving around",
  "the same place",
  "right so you can see what's happened is",
  "the learning rates too high so we could",
  "just go in here and drop it down and run",
  "it some more getting pretty close now",
  "right so you can see how you still need",
  "learning rate annealing even with Adam",
  "okay so that spreadsheets fun to play",
  "around with I do have a Google sheets",
  "version of basic SGD that actually works",
  "and the macros work and everything",
  "Google sheets is so awful and I went so",
  "insane making that work I gave up I'm",
  "making the other ones work so I'll share",
  "a link to the Google sheets version oh",
  "my god they do have a macro language but",
  "it's just ridiculous so anyway if",
  "somebody feels like fighting it to",
  "actually get all the other ones to work",
  "we'll work it just assistant so maybe",
  "somebody can get this working on Google",
  "sheets too okay so that's weight decay",
  "and Adam and Adam is amazingly fast and",
  "we let's go back to this one but we",
  "don't tend to use op teamed-up",
  "whatever and create the optimizer",
  "ourselves and all that stuff because",
  "instead we had to use learner but learn",
  "is just doing those things for you but",
  "again there's no magic right so if you",
  "create and learner you say here's my",
  "data bunch here's my PI torch and n dot",
  "module instance here's my loss function",
  "and here are my metrics remember the",
  "metrics are just stuff to print out",
  "that's it right then you just get a few",
  "nice things like learned at LR fine",
  "starts working and it starts recording",
  "this and you can say fit one cycle",
  "instead of just fit but like these",
  "things really help a lot like by using",
  "the floating rate finder I found a good",
  "learning rate and then like look at this",
  "my loss here 0.13 here I wasn't getting",
  "much beneath point five right so these",
  "these tweets make huge differences not",
  "tiny differences and this is still just",
  "one one epoch now what does fit one",
  "cycle do what does it really do this is",
  "what it really does right and we've seen",
  "this chart on the left before just to",
  "remind you this is plotting the learning",
  "rate per batch right remember Adam has a",
  "learning rate and we use Adam by default",
  "or minor variation which we might try to",
  "talk about so the learning rate starts",
  "really low and it increases about half",
  "the time and then it decreases about",
  "half the time because at the very start",
  "we don't know where we are right so",
  "we're in some part of function space",
  "it's just bump years or hell all right",
  "so if you start",
  "jumping around those bumps have big",
  "gradients and it will throw you into",
  "crazy parts of the space right so start",
  "slow and then you'll gradually move into",
  "parts of the weight space that you know",
  "and they're kind of sensible and as you",
  "get to the points where they're sensible",
  "you can increase the learning rate you",
  "know because the the gradients jedd",
  "actually in the direction you want to go",
  "right and then as we've discussed a few",
  "times as you get close to the final",
  "answer",
  "you need to anneal your learning rate to",
  "hone in on it but here's the interesting",
  "thing on the left is the momentum plot",
  "and actually every time our learning",
  "rate is small our momentum is high why",
  "is that because if you I do have a",
  "learning small learning rate but you",
  "keep going in the same direction you may",
  "as well go faster right but if you're",
  "jumping really far don't like jump jump",
  "really far because it's going to throw",
  "you off right and then as you get to the",
  "end again you're fine tuning in but",
  "actually if you keep going the same",
  "direction again and again go faster yeah",
  "so this combination is called",
  "one cycle and it's just this amazing",
  "like it's a simple thing but it's",
  "astonishing like this can help you get",
  "what's called super convergence that can",
  "let you train ten times faster now this",
  "was just last year's paper when some of",
  "you may have seen the interview with",
  "Leslie Smith that I did last week",
  "amazing guy incredibly humble and also I",
  "should say somebody who is doing",
  "groundbreaking research well into his",
  "60s and all of these things are",
  "inspiring I'll show you something else",
  "interesting when you plot the losses",
  "with fast AI it doesn't look like that",
  "it looks like that why is that because",
  "fast AI calculates the exponentially",
  "weighted moving average of the losses",
  "for you all right so this this concept",
  "of exponentially weighted stuff it's",
  "just really handy and I use it all the",
  "time",
  "and one of the things that is to make it",
  "easier to read these charts okay it does",
  "mean that these charts from faster I",
  "might be kind of an epoch or two sorry a",
  "batch or two behind",
  "where they should be you know there's",
  "that slight downside when you use an",
  "exponentially weighted moving average is",
  "you've got a little bit of history in",
  "there as well",
  "but I can make it much easier to see",
  "what's going on so we're now at a point",
  "coming to the end of this collab in",
  "tabular section where we're going to try",
  "to understand all of the code in our",
  "tabular model so remember the tabular",
  "model use this data set called adult",
  "which is trying to predict who's going",
  "to make more money it's a classification",
  "problem and we've got a number of",
  "categorical variables and a number of",
  "continuous variables so the first thing",
  "we realize is we actually don't know how",
  "to predict a categorical variable yet",
  "because so far we did some hand waving",
  "around the fact that our loss function",
  "was an n dot cross entropy loss what is",
  "that",
  "let's find out and of course we're going",
  "to find out by looking at Microsoft",
  "Excel so cross-entropy loss is just",
  "another loss function well you already",
  "know 1 loss function which has means",
  "grid error",
  "y hat minus y squared ok so that's not a",
  "good loss function for us because in our",
  "case we have like for M list 10 possible",
  "digits and we have 10 activations each",
  "with a probability of that digit okay so",
  "we need something we're predicting the",
  "right thing correctly and confidently",
  "should have very little loss predicting",
  "the wrong thing confidently should have",
  "a lot of loss so that's what we want",
  "okay",
  "so here's an example here is cat versus",
  "dog",
  "one hot encoded ok and here are my two",
  "activations for each one from some model",
  "that I built probability cat probability",
  "dog this one's not very confident of",
  "anything this one's very confident",
  "perfect being a cat that's right this",
  "one's very confident for being a cat and",
  "it's wrong so we want to loss that",
  "for this one should be a moderate loss",
  "because not predicting anything",
  "confidently is not really what we want",
  "so here's a point in three",
  "this thing's predicting the correct",
  "thing very confidently",
  "so 0.01 there's things predicting the",
  "wrong thing very confidently so one so",
  "how do we do that this is the cross",
  "entropy loss and it is equal to whether",
  "it's a cat multiplied by log of the",
  "probability of cat well this is actually",
  "an activation so I should say so it's",
  "multiplied by the log of the cat",
  "activation negative that - is it a dog",
  "times the log of the dog activation and",
  "that's it",
  "okay so in other words it's the sum of",
  "all of your one hot encoded variables",
  "times all of your activations so",
  "interestingly these ones here exactly",
  "the same numbers as these ones here but",
  "I've written it differently I've written",
  "up with an if function because it's",
  "exactly this quiz because the zeros",
  "don't actually add anything all right so",
  "actually it's exactly the same as saying",
  "if it's a cat then take the log of",
  "cattiness and if it's a dog yes or",
  "otherwise take the log of one minus",
  "cattiness in other words the log of dog",
  "Eunice so the sum of the one hot encoded",
  "times the activations is the same as an",
  "if function which if you think about it",
  "it's actually because this is just a",
  "matrix multiply this is we now know from",
  "our from our embedding discussion that's",
  "the same as an index lookup so you can",
  "also - do cross entropy you can also",
  "just look up the log of the activation",
  "for the correct answer now that's only",
  "going to work",
  "if these rows add up to one and this is",
  "one reason that you can get screwy",
  "cross-entropy numbers is this way I said",
  "you press the wrong button",
  "if they don't add up to 1 you've got a",
  "trouble so how do you make sure that",
  "they add up to 1 you make sure they add",
  "up to 1 by using the correct activation",
  "function in your last layer and the",
  "correct activation function to use for",
  "this is softmax softmax is an activation",
  "function where all of the activations",
  "add up to 1 all of the activations are",
  "greater than 0 and all of the",
  "activations are less than 1 so that's",
  "what we want right that's what we need",
  "how do you do that well let's say we",
  "were predicting one of five things cat",
  "dog plane fish building and these were",
  "the numbers that came out of our neural",
  "net for one set of predictions well what",
  "if I did",
  "e to the power of that so that's one",
  "step in the right direction because e to",
  "the power of something is always bigger",
  "than zero so there's a bunch of numbers",
  "that are always bigger than zero here's",
  "the sum of those numbers here is a to",
  "the number divided by the sum of e to",
  "the number now this number is always",
  "less than one right because all of the",
  "things were positive so you can't",
  "possibly have one of the pieces be",
  "bigger than 100 percent of its sum okay",
  "and all of those things must add up to",
  "one right because each one of them was",
  "just that percentage of the total so",
  "that's it",
  "so this thing softmax is equal to e to",
  "the activation divided by the sum of e",
  "to the activations",
  "that's called softmax and so when we're",
  "doing single label multi-class",
  "classification you generally want",
  "softmax as your activation function and",
  "you generally want cross-entropy",
  "as your loss now because these things go",
  "together in such",
  "friendly ways play torch we'll do them",
  "both for you all right so you might have",
  "noticed that in this eminent example I",
  "never added a soft max here and that's",
  "because if you ask for cross entropy",
  "loss it actually does the softmax in",
  "inside the loss function so it's not",
  "really just cross entropy loss it's",
  "actually softmax then cross entropy loss",
  "so you've probably noticed this but",
  "sometimes your predictions from your",
  "models will come out looking more like",
  "this pretty big numbers with negatives",
  "in rather than this numbers between",
  "norton 1 that add up to 1",
  "the reason would be that pi torch",
  "it's a pi torch model that doesn't have",
  "a softmax in because we're using cross",
  "entropy loss and so you might have to do",
  "the softmax for it fast AI is getting",
  "increasingly good at knowing when this",
  "is happening generally if you're using a",
  "loss function that we recognize when you",
  "get the predictions we will try to add",
  "the softmax in there for you but if you",
  "particularly if you're using a custom",
  "loss function that you know might call",
  "and end up crossing entropy loss behind",
  "the scenes or something like that you",
  "might find yourself with this situation",
  "we only have 3 minutes less but I'm",
  "going to point something out to you",
  "which is that next week when we finish",
  "off tabular which we'll do in like the",
  "first 10 minutes this is forward in",
  "tabular and it basically goes through a",
  "bunch of embeddings right it's going to",
  "call each one of those embeddings E and",
  "you can use it like a function of course",
  "so it's going to pass a nitch",
  "categorical variable to each embedding",
  "it's going to concatenate them together",
  "into a single matrix it's going to then",
  "call a bunch of layers which are",
  "basically a bunch of linear layers and",
  "then it's going to do our sigmoid trick",
  "and then there's only",
  "two new things we'll need to learn one",
  "is dropout and the other is the end",
  "can't better not and these are two",
  "additional regularization strategies",
  "right there are basically better on does",
  "more than just regularization but",
  "amongst other things it does",
  "regularization and the basic ways you",
  "regular as your model weight decay batch",
  "norm and dropout okay and then you can",
  "also avoid overfitting using something",
  "called data augmentation so better",
  "Normand dropout we're going to touch on",
  "at the start of next week and we're also",
  "going to look at data augmentation and",
  "then we're also going to look at what",
  "convolutions are and we're going to",
  "learn some new computer vision",
  "architectures and some new computer",
  "vision applications but basically we're",
  "very nearly there you already know how",
  "the entirety of collab py first light",
  "club works you know what why it's there",
  "and what it does and you're very close",
  "to knowing what the entirety of tabular",
  "model does and this tabular model is",
  "actually the one that if you run it on",
  "rossmann you'll get the same answer that",
  "I showed you in that paper you'll get",
  "that second place result in fact even a",
  "little bit better I'll show you next",
  "week if I remember how I actually ran",
  "some additional experiments where I",
  "figured out some minor tweaks that can",
  "do even slightly better than that so",
  "yeah we'll see you next week thanks very",
  "133:25": "much and enjoy the smoke outside"
}