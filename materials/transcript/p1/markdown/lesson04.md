{
"okay welcome to lesson 4 we are going to",
"finish our journey through these kind of",
"key applications we've already looked at",
"a range of vision applications we've",
"looked at classification localization",
"image regression we've briefly touched",
"on NLP we're going to do a deeper dive",
"into NLP transfer learning today we're",
"going to then look at tabular data and",
"we're going to look at collaborative",
"filtering which are both super useful",
"applications and then we're going to",
"take a complete u-turn we're going to",
"take that collaborative filtering",
"example and dive deeply into it to",
"understand exactly what's happening",
"mathematically exactly what's happening",
"in the computer and we're going to use",
"that to gradually go back in reverse",
"order through the applications again in",
"order to understand exactly what's going",
"on behind the scenes of all of those",
"applications before we do somebody on",
"the forum was kind enough to point out",
"that when we compared ourselves to the",
"what we think might be the",
"state-of-the-art but was recently the",
"state of the art for camford there",
"wasn't a fair comparison because the",
"paper actually used a small subset of",
"the classes and we used all of the",
"classes so jason in our study group was",
"kind enough to rerun the experiments",
"with the correct subset of classes from",
"the paper and our accuracy went up to 94",
"percent compared to 91.5% in the paper",
"so I think that's a really cool result",
"and a great example of how some pretty",
"you know pretty much just using the",
"defaults nowadays can can get you far",
"beyond what was the best of a year or",
"two ago now it's certainly the best last",
"year when we were doing this course",
"because we started it quite quite",
"intensely so that's really exciting so",
"what I wanted to start with is going",
"NLP a little bit to understand really",
"what was going on there so first of all",
"a quick review",
"so remember NLP is natural language",
"processing it's about taking text and",
"doing something with it and text",
"classification is particularly useful",
"kind of practically useful applications",
"it's what we're going to start off",
"focusing on because classifying a text",
"classifying a document can be used for",
"anything from spam prevention to",
"identifying fake news to finding a",
"diagnosis or medical reports finding",
"mentions of your product in Twitter so",
"on and so forth",
"so it's pretty interesting and actually",
"there was a great example there was a",
"great example during the week from one",
"of our students who is a lawyer and he",
"mentioned on the forum that he had a",
"really great results from classifying",
"legal texts using this NLP approach and",
"I thought this was a great example so",
"this is the post that they presented at",
"an academic conference this week",
"describing the approach and actually",
"this series of three steps that you see",
"here and I'm sure you recognize these",
"classification matrix this series of",
"three steps here is what we're going to",
"start by digging into so we're going to",
"start out with a movie review like this",
"one and going to decide whether it's",
"positive or negative sentiment about the",
"movie that is the problem we have in the",
"training set 25,000 movie reviews so",
"we've got 25,000 movie reviews and for",
"each one we have like one bit of",
"information they liked it or they didn't",
"like it and that's we're going to look",
"into a lot more detail of today and in",
"the current lessons our neural networks",
"remember they're just a bunch of matrix",
"multiplies and simple nonlinearities",
"particularly replacing negatives with",
"zeros those weight matrices start out",
"random and so if you start out with with",
"some random parameters and try to train",
"those parameters to learn how to",
"recognize positive versus negative movie",
"reviews",
"you only have 20 literally 25,000 ones",
"and zeros to actually tell you I like",
"this one I don't like that one that's",
"clearly not enough information to learn",
"basically how to speak English how to",
"speak English well enough to recognize",
"they liked this or they didn't like this",
"and sometimes that can be pretty nuanced",
"right the English language often",
"particularly would like movie reviews",
"people because these are like online",
"movie reviews on IMDB people can often",
"like use sarcasm it could be really",
"quite tricky so it for a long time until",
"in fact until very recently like this",
"year neural nets didn't do a good job at",
"all of this kind of classification",
"problem and and that was why there's not",
"enough information available so the",
"trick hopefully you can all guess it's",
"to use transfer learning it's always the",
"trick so last year in this course I",
"tried something crazy which was I",
"thought what if I try transfer learning",
"to demonstrate that it can work for an",
"LP as well and and I tried it out and it",
"worked extraordinarily well and so here",
"we are a year later and transfer",
"learning in NLP is absolutely the the",
"hit thing now and so I'm going to",
"describe to you what happens the key",
"thing is we're going to start with the",
"same kind of thing that we used for",
"computer vision a pre trained model",
"that's been trained to do something",
"different to what we're doing with it",
"and so for imagenet that was originally",
"built as a model to predict which of a",
"thousand categories each photo falls",
"into and people then fine-tune that for",
"all kinds of different things as we've",
"seen so we're going to start with a pre",
"trained model that's going to do",
"something else",
"not movie review classification we're",
"going to start with a pre train model",
"which is called a language model a",
"language model is a very very specific",
"meeting in NLP and it's this a language",
"model is a model that learns to predict",
"the next word of a sentence and to",
"predict the next word of a sentence you",
"actually have to know quite a lot about",
"English assuming you're doing it in",
"English and quite a lot of world",
"knowledge by world knowledge a different",
"example here's your language model and",
"as read I'd like to eat a hot what",
"obviously",
"dog right it was a heart what",
"probably day right now previous",
"approaches to NLP use something called",
"engrams largely which is basically",
"saying how often do these pairs or",
"triplets of words tend to appear next to",
"each other",
"and engrams are terrible at this kind of",
"thing as you can see there's not enough",
"information here to decide what the next",
"word probably is but with a neural net",
"you're absolutely can so here's the nice",
"thing if you train a neural net to",
"predict the next word of a sentence then",
"you actually have a lot of information",
"rather than having a single bit for",
"every 2000 word movie review liked it or",
"different like it every single word you",
"can try and predict the next word so in",
"a 2,000 word movie review",
"there are 1999 opportunities to predict",
"the next word better still you don't",
"just have to look at movie reviews",
"because really the hard thing isn't so",
"much is does this person like the movie",
"or not but how do you speak English and",
"so you can learn how do you speak",
"English roughly from some much bigger",
"set of documents and so what we did was",
"we started with Wikipedia and Stephen",
"Merritt II and some of his colleagues",
"built something called the wiki text 103",
"data set which is simply a subset of",
"most of the largest articles from",
"Wikipedia with a little bit of",
"pre-processing that's available for",
"download",
"and so you're basically grabbing",
"Wikipedia and then I built a language",
"model on all of Wikipedia right so I've",
"just built a neural net which would",
"predict the next word in every",
"significantly sized Wikipedia article",
"and that's a lot of information if I",
"remember correctly it's something like a",
"billion tokens all right so we've got a",
"billion separate things to predict every",
"time we make a mistake on one of those",
"predictions we get the loss can get we",
"get gradients from that and we can",
"update our weights and make them better",
"and better until we can get pretty good",
"at predicting the next word of Wikipedia",
"why is that useful because at that point",
"I've got a model that knows probably how",
"to complete sentences like this and so",
"it knows quite a lot about English and",
"quite a lot about how the world works",
"what kinds of things tend to be hot in",
"different situations for instance I mean",
"ideally it would learn things like in",
"1996 in a speech to the United Nations",
"United States President blah said now",
"that would be a really good language",
"model because it would actually have to",
"know who was this United States",
"president in that year so like getting",
"really good at training language models",
"is a great way to learn a lot about or",
"teacher neural-net",
"a lot about you know what is our world",
"what's in our world how do things work",
"in our world so it's a really",
"fascinating topic and it's actually one",
"that philosophers have been studying for",
"hundreds of years now there's actually a",
"whole theory of philosophy which is",
"about like what can be learned from",
"studying language alone so it turns out",
"empirically quite a lot and so here's",
"the interesting thing you can start by",
"training a language model on all of",
"Wikipedia and then we can make that",
"available to all of you just like a pre",
"trained imagenet model her vision we've",
"now made available a pre trained",
"wikitext model for NLP not because it's",
"particularly useful of itself predicting",
"the next word of sentences is somewhat",
"useful but not normally what we want to",
"do but it tells us it's a it's a model",
"that understands a lot about language",
"and a lot about what language describes",
"so then we can take that and we can do",
"transfer learning to create a new",
"language model that's specifically good",
"at predicting the next word of movie",
"reviews so if we can build a language",
"model that's good at predicting the next",
"word of movie reviews pre trained with",
"the wikitext model right then that's",
"going to understand a lot about my",
"favorite actor is Tom who write or you",
"know I thought the photography was",
"fantastic but I wasn't really so happy",
"about the director whatever right it's",
"going to learn a lot about specifically",
"how movie reviews are written it'll even",
"learned things like what are the names",
"of some popular movies so that would",
"then mean we can still use a huge corpus",
"of lots of movie reviews even if we",
"don't know whether they're positive or",
"negative right to learn a lot about how",
"movie reviews are written so for all of",
"this pre training and all of this",
"language model fine-tuning we don't need",
"any labels at all",
"it's what the researcher Yan Lacan calls",
"self supervised learning in other words",
"it's a classic supervisor model we have",
"labels",
"right but the labels are not things that",
"somebody else have created they're kind",
"of built into the data set itself so",
"this is really really neat because at",
"this point we've now got something",
"that's good at understanding movie",
"reviews and we can fine-tune that with",
"transfer learning to do the thing we",
"want to do which in this case is to",
"classify movie reviews to be positive or",
"negative and so my hope was when I tried",
"this last year that at that point 25,000",
"ones and zeros would be enough feedback",
"to fine-tune that model and it turned",
"out it absolutely was all right Rachel",
"let's go with a question does the",
"language model approach work for text in",
"forums that are informal English",
"misspelled words are slang or short form",
"like S six instead of Samsung S six yes",
"absolutely it does particularly if you",
"start with your wikitext model and then",
"fine tune it with your we call it a",
"target corpus",
"so your tech or purse is just a bunch of",
"documents right could be emails or",
"tweets or medical reports or whatever so",
"you could fine tune it so it can learn a",
"bit about the specifics of the slang if",
"you know or abbreviations or whatever",
"that didn't appear in the full corpus",
"and so interestingly this is one of the",
"big things that people were surprised",
"about when we did this research last",
"year",
"people thought that learning from",
"something like Wikipedia wouldn't be",
"that helpful because it's not that",
"representative of how people tend to",
"write but it turns out it's extremely",
"helpful because there's a much bigger",
"difference between Wikipedia and random",
"words than there is between like",
"wikipedia and read it say so it kind of",
"gets you 99% of the way there",
"so these language models themselves can",
"be quite powerful so for example there",
"was a blog post from what are they",
"called SwiftKey swiftlet SwiftKey the",
"folks that do the mobile phone",
"predictive text keyboard and they",
"described how they kind of rewrote their",
"their underlying model to use neural",
"nets so and now this was a year or two",
"ago now most phone keyboards seem to do",
"this you'll be typing away on your",
"mobile phone and in the predictions",
"there'll be something telling you what",
"words you might want next so that's a",
"language model in your phone another",
"example was the researcher on drake",
"apathy who's now runs all this stuff at",
"Tesla Beck when he has a PhD student he",
"created a language model of text in",
"latex documents and created these",
"automatic generation of latex documents",
"that then became these kind of",
"automatically generated papers that's",
"pretty cute so we're not really that",
"interested in the output of the language",
"model ourselves we're just interested in",
"it because it's helpful with this",
"process so um we briefly looked at the",
"process last week so that's like just",
"have a reminder right that the basic",
"process is we're going to start with",
"the the data in some format so for",
"example we've prepared a little IMDB",
"sample that you can use where it's in",
"CSV file so you can read it in with",
"pandas and see there's negative or",
"positive the text of each movie review",
"and boolean of is it in the validation",
"set or the training set so there's an",
"example of a movie review and so you can",
"just go text date a bunch from CSV to",
"grab a language model specific data",
"bunch and then you can create a learner",
"from that in the usual way and fit it",
"you can save there's a data bunch which",
"means that the pre-processing that is",
"done you don't have to do it again you",
"can just load it so what goes on behind",
"the scenes well what happens behind the",
"scenes if we now load it as a",
"classification data bunch that's going",
"to allow us to see the labels as well",
"then as we described it basically",
"creates a separate unit we call it a",
"token for each separate part of a word",
"so most of them are just small words but",
"sometimes if it's like an apostrophe s",
"from its you know get its own token",
"every bit of punctuation tends to get",
"its own pro cone like a comma or a",
"full-stop and so forth and then the next",
"thing that we do is a numerical ization",
"which is where we find what are all of",
"the unique tokens that appear here and",
"we create a big list of them here's the",
"first ten in order of frequency and that",
"big list of unique possible tokens is",
"called the vocabulary no it's called a",
"vocab and so what we then do is we",
"replace the tokens with the ID of where",
"is that token in the vocab okay and that",
"that's numerical ization here's the",
"thing though as you'll learn every word",
"in our vocabulary and so to avoid that",
"weight matrix getting too huge we",
"restrict the vocab to no more than by",
"default 60,000 words and if a word",
"doesn't",
"appear more than two times we don't put",
"it in the vocab either so we kind of",
"keep the vocab to a reasonable size in",
"that way and so when you see these xx",
"UNK that's an unknown token so when you",
"see those unknown tokens it just means",
"this was something that was not a common",
"enough word to appear in our vocab okay",
"so there is the numerical lowest version",
"we also have a couple of other special",
"tokens like XX field this is a special",
"thing where if you've got like title",
"summary abstract body like separate",
"parts of a document each one will get a",
"separate field and so they will get",
"numbered also you'll find if there's",
"something in all caps it gets lower",
"cased and a token called xx cap will get",
"added to it personally I more often use",
"the data block API because you get kind",
"of there's less to remember about",
"exactly what data bunch to use and what",
"parameters and so forth and it can be a",
"bit more flexible so another approach to",
"doing this is to just decide what kind",
"of list you're creating so what's your",
"independent variable so in this case my",
"independent variable is text",
"what is it coming from a CSV how do you",
"want to split it into validation versus",
"training so in this case column number",
"two was the is validation flag how do",
"you want to label it with positive or",
"negative sentiment for example so column",
"0 had that and then turn that into a",
"database that's going to do the same",
"thing okay",
"so now let's grab the whole data set",
"which has 25,000 reviews in trading",
"25,000 reviews in validation and then",
"50,000 what they call unsupervised movie",
"reviews so fifty thousand movie reviews",
"that haven't been scored at all so there",
"it is positive negative unsupervised so",
"we're going to start as we described",
"with the language model now the good",
"news is we don't have to train the",
"wikitext 103 language model not that",
"it's difficult you can use exactly the",
"same steps you see here just download",
"the wiki text 103 corpus and run the",
"same code but it takes two or three days",
"on a decent GPU so not much point you",
"doing it you may as well start with",
"hours even if you've got a big corpus of",
"like medical documents or legal",
"documents you should still start with",
"wikitext 103 like there's just no reason",
"to start with random weights it's always",
"good to use transfer learning if you can",
"so we're going to start then at this",
"point which is fine-tuning our IMDB",
"language language model so we can say",
"okay it's a list of text files and the",
"full IMDB actually is not in a CSV each",
"each document is a separate text file so",
"that's why we use a different",
"constructor for our independent variable",
"text files list say where it is and in",
"this case we have to make sure we just",
"don't include the train and test folders",
"and we randomly split it by 0.1 now this",
"is interesting",
"10% why are we randomly splitting it by",
"10% rather than using the predefined",
"train and test they gave us this is one",
"of the cool things about transfer",
"learning even though our test set or a",
"validation set has to be held aside it's",
"actually only the labels that we have to",
"keep aside so we're not allowed to use",
"the labels and the test set so if you",
"think about saying like a capital",
"competition you certainly can't use the",
"labels because they don't even give them",
"to you",
"but you can certainly use the",
"independent variables so in this case",
"you could absolutely use the text that",
"is in the the test set to train your",
"language model so this is a good trick",
"right is actually when you do the",
"language model concatenate the training",
"and test set together and then just let",
"out a smaller validation set so you've",
"got more data to train your language",
"model so that's a little trick and so if",
"you're doing NLP stuff on kaggle for",
"example or you know you've just got a",
"smaller subset of labelled data make",
"sure that you use all of the text you",
"have to train",
"your language model because there's no",
"reason not to how are we going to label",
"it well remember a language model kind",
"of has its own labels so the text itself",
"is label so label for language model",
"does that for us and create a data bunch",
"and save it and that takes a few minutes",
"to tokenize and numerical s so since it",
"takes a few minutes we save it later on",
"you can just load it no need to run that",
"again so here's what it looks like and",
"at this point things are going to look",
"very familiar we create a learner but",
"instead of creating a CNN learner we're",
"going to create a language model learner",
"so behind the scenes this is actually",
"not going to create a CNN a",
"convolutional neural network it's going",
"to create an AR and in a recurrent",
"neural network so we're going to be",
"learning exactly how they're built over",
"the coming lessons but in short they're",
"the same basic structure the input goes",
"into a weight matrix a matrix multiply",
"that then you replace the negatives with",
"zeroes and it goes into another matrix",
"multiply and so forth a bunch of times",
"so it's the same basic structure so as",
"usual when we create a learner you have",
"to pass in two things the data so here's",
"our language model data and in this case",
"what pre-trade model we want to use and",
"so here the pre-trade model is the",
"wikitext 1:03 model that will be",
"downloaded for you from first AI if you",
"haven't used it before just like the",
"same thing with things like image net",
"pre-trained models are downloaded for",
"you this here sets the amount of dropout",
"we haven't talked about that yet we've",
"talked briefly about this idea that",
"there's something called regularization",
"and you can reduce the regularization to",
"avoid underfitting",
"so for now just know that by using a",
"number lower than 1 is because when I",
"first tried to run this I was under",
"fitting and so if you reduced that",
"number then it will avoid under fitting",
"okay so we've got a loner we can LR find",
"looks pretty standard and so then we can",
"see it one cycle",
"so what's happening here is we are just",
"fine-tuning the last layers so normally",
"after we fine-tune the last layers the",
"next thing we do is we go",
"unfreeze and train the whole thing and",
"so here it is",
"unfreeze and train the whole thing and",
"as you can see even on a pretty beefy",
"GPU that takes 2 or 3 hours and in fact",
"I'm still under fitting all right so I",
"probably tonight I might train it",
"overnight and train to a little bit",
"better because you can see oh I guess",
"I'm not underfitting oh I'm guessing I",
"could probably train this a bit longer",
"because you can see the accuracy hasn't",
"started going down again that's I",
"wouldn't mind trying to train that a bit",
"longer but the accuracy it's interesting",
"point three means you know we're",
"guessing the next word of the movie",
"review correctly about a third of the",
"time so that sounds like a pretty high",
"number the idea that you can actually",
"guess the next word that often so it's a",
"good sign that my language model is",
"doing pretty well for kind of more",
"limited domain documents like medical",
"transcripts and legal transcripts you'll",
"often find this accuracy Gertz gets a",
"lot higher so sometimes this can be even",
"50% or more but you know point three or",
"more is is pretty good so you can now",
"run learn dot predict and pass in the",
"start of a sentence and it will try and",
"finish off that sentence for you now I",
"should mention we this is not designed",
"to be a good text generation system this",
"is really more designed to kind of check",
"that it seems to be creating something",
"that's vaguely sensible there's a lot of",
"tricks that you can use to generate much",
"higher quality text none of which we're",
"using here right but you can kind of see",
"that it's it's it's you know certainly",
"not random words that it's generating it",
"sounds vaguely English like even though",
"it doesn't make any sense",
"so at this point we have a movie review",
"model so now we're gonna save that in",
"order to load it into our classifier to",
"be a pre trained model for the",
"classifier but I actually don't want to",
"save the whole thing a lot of this you",
"know kind of the second half as we'll",
"learn the second half of the language",
"model is all about predicting the next",
"word rather about rather than about",
"understanding the sentence so far so the",
"bit which is specifically about",
"understanding the sentence so far is",
"called the encoder so I just saved that",
"alright so and again we're going to",
"learn the details of this there coming",
"weeks",
"Rena's going to save the encoder so the",
"bit that understands the sentence rather",
"than the bit that generates the word so",
"now we're ready to create our classifier",
"so step one as per usual is to create a",
"data bunch and we're going to do",
"basically exactly the same thing bring",
"it in okay and here's our path but we",
"want to make sure that it uses exactly",
"the same vocab that are used for the",
"language model if word number 10 was ver",
"in the language model we need to make",
"sure that word number 10 is the in the",
"classifier because otherwise the",
"pre-trained model is going to be totally",
"meaningless so that's why we pass in the",
"vocab from the language model to make",
"sure that this data bunch is going to",
"have exactly the same vocab that's an",
"important step split by folder and this",
"time label so remember the last time we",
"had split randomly okay but this time we",
"need to make sure that the labels of the",
"test set are not touched so we split by",
"folder and then this time we label it",
"not for a language model but we label",
"these classes and then finally create a",
"data bunch and remember sometimes you'll",
"find that you ran out of GPU memory this",
"will very often happen to you if you so",
"I was running this in an 11 gig machine",
"so you should make sure this numbers a",
"bit lower if you run out of memory you",
"may also want to make sure you restart",
"the notebook and kind of started just",
"from here so batch size 50 is as high as",
"I could get on an 11 gig card if you are",
"using a p2 or p3 on Amazon or the kad on",
"Google for example I think you'll get 16",
"gigs so you might be able to make this a",
"bit higher cut it up to 64 so you can",
"find whatever batch size fits on your",
"card so here's our data bunch as we saw",
"before and the labels so this time",
"rather than creating a language model",
"learner we're creating a text classifier",
"learner but again same thing pass in the",
"data that we want figure out how much",
"regularization we need again if you're",
"overfitting then you can increase this",
"number if you're under fitting you can",
"decrease the number and most importantly",
"load in a pre trained model and remember",
"specifically it's just this this half of",
"the model called the encoder which is",
"the bit that we want to load in and",
"freeze now I find find the learning rate",
"and fit for a little bit and we're",
"already up nearly to 92% accuracy after",
"less than three minutes of training so",
"this is a nice thing in your particular",
"domain whether it be law or medicine or",
"journalism or government or whatever you",
"probably only need to train your domains",
"language model once and that might take",
"you know overnight to Train well but",
"once you've got it you can now very",
"quickly create all kinds of different",
"classifiers and models with that in you",
"know in this case already a pretty good",
"model after three minutes right so so",
"when you first start doing this you",
"might find it a bit it's like annoying",
"that your first models take four hours",
"more or more to create that language",
"model but the key thing to remember is",
"you only have to do that once for your",
"entire kind of domain of stuff that",
"you're interested in and then you can",
"build lots of different classifiers and",
"other models on top of that in a few",
"minutes okay",
"all right so we can save that to make",
"sure you directly run it again and then",
"here's something interesting now I'm",
"gonna explain this more",
"just a few minutes I'm not gonna say",
"unfreeze instead of going to save fries",
"- and what that says is unfreeze the",
"last two layers don't unfreeze the whole",
"thing and so we've just found it really",
"helps with these text classification not",
"to unfreeze the whole thing but to",
"unfreeze one layer at a time",
"so unfreeze the last two layers train it",
"a little bit more and freeze the next",
"layer again train a little bit more",
"unfreeze the whole thing train it a",
"little bit more you'll also see I'm",
"passing in this thing Momentum's equals",
"point eight point seven we're going to",
"learn exactly what that means in the",
"next week or two probably next week but",
"for now and we may even automate it so",
"maybe by the time you watch the video of",
"this this won't even be necessary",
"anymore basically we found for training",
"recurrent neural networks our own ends",
"it really helps to decrease the momentum",
"a little bit so that's what that is",
"so that gets us a ninety four point for",
"accuracy after about half an hour or",
"less of training actually quite a lot",
"less of training the actual classifier",
"and we can actually get this quite a bit",
"better with a few tricks I don't know if",
"we learned all the tricks this part it",
"might be next part but even this very",
"simple kind of standard approach is",
"pretty great if we compare it to last",
"year's state of the art on IMDb's is",
"from The Cove paper from McCann at L at",
"Salesforce Research their paper was",
"ninety one point eight percent accurate",
"in the best paper they could find they",
"found a fairly domain-specific sentiment",
"in analysis paper from 2017 they've got",
"ninety four point one and here we've got",
"ninety four point four and the best",
"models I've been able to build since",
"have been about ninety five ninety five",
"point one so if you're looking to do",
"text classification this you know really",
"standardized transfer learning approach",
"works work super well any questions",
"Rachel okay so that was that was an LP",
"and we'll be learning more about NLP",
"later in this course but now I wanted to",
"switch over and look at tabular now",
"tabular data is pretty interesting",
"because it's the stuff that for a lot of",
"you is actually what you use day-to-day",
"at work in spreadsheets and relational",
"databases just come close I guess hey so",
"where does the magic number of 2.6 to",
"the fourth in the learning rate come",
"from yeah good question so the learning",
"rate is various various things divided",
"by 2.6 to the fourth",
"the reason it's to the fourth you will",
"learn about at the about the end of",
"today so let's focus on the 2.6 why 2.6",
"basically this as we're as we're going",
"to see in more detail later day this",
"this number there are the difference",
"between the bottom of the slice and the",
"top of the slice is basically what's the",
"difference between how quickly the",
"lowest layer of the model learns versus",
"the highest layer of their model learns",
"so this is called discriminative",
"learning rates and so really the",
"question is like as you go from layer to",
"layer",
"how much do I decrease the learning rate",
"by when we found out that for NLP are n",
"ends that the answer is 2.6 how do we",
"find out that it's 2.6 I ran lots and",
"lots of different models like a year ago",
"or so using lots of different sets of",
"hyper parameters of various types drop",
"out learning rates and discriminative",
"learning rate and so forth and then I",
"created something called a random forest",
"which is the kind of model where I",
"attempted to predict how accurate my NLP",
"classifier would be based on the hyper",
"parameters and then I",
"used random forest interpretation",
"methods to basically figure out what the",
"optimal parameter settings were and I",
"found out that the answer for this",
"number was 2.6 so that's actually not",
"something I've published or I don't",
"think I've even talked about it before",
"so there's a new piece of information",
"you can actually a few months after I",
"did this I think David marady and",
"somebody else did publish a paper",
"describing a similar approach so the",
"basic idea may be out there already some",
"of that idea comes from a researcher",
"named Frank Hatter and one of his",
"collaborators they did some interesting",
"work showing how you can use random",
"forests to actually find optimal",
"hyperparameters so it's kind of a neat",
"trick you know a lot of people are very",
"interested in this in court auto ml",
"which is this idea of like building",
"models to figure out how to train your",
"model we're not big fans of it on the",
"whole but we do find that building",
"models to better understand how your",
"hyper parameters work and then finding",
"like those rules of thumb like oh",
"basically it can always be two-point-six",
"quite helpful so that's just something",
"we're kind of been playing with okay so",
"yeah so let's talk about tabular data so",
"tabular data such as you might see in a",
"spreadsheet or a relational database you",
"know or a financial report it can",
"contain all kinds of different things it",
"can contain all kinds of different",
"things and I kind of tried to make a",
"little list of some of the kinds of",
"things that I've seen tabular data",
"analysis used for using neural nets for",
"analyzing tabular data is or at least",
"last year when I first presented this",
"was maybe we started this two years ago",
"yeah when we first presented this people",
"were deeply skeptical and they thought",
"it was a terrible idea to use neural",
"nets to analyze tabular data because",
"like everybody knows that you should use",
"logistic regression or random forests or",
"gradient boosting machines all of which",
"have their place but certain certain",
"types of things but since that time you",
"know it's become clear that the commonly",
"held wisdom is is wrong it's not true",
"that neural nets are not useful for",
"tabular data in fact the extremely",
"useful now we've shown this in in quite",
"a few of our courses but what's really",
"kind of also helped is that some really",
"effective organizations have started",
"publishing papers and posts and stuff",
"describing how they've been using neural",
"nets for analyzing tabular data um one",
"of the key things that comes up again",
"and again is that although feature",
"engineering doesn't go away it certainly",
"becomes simpler right so Pinterest for",
"example replaced the gradient boosting",
"machines that they were using to decide",
"how to put stuff on their homepage with",
"neural nets and they presented at a",
"conference this approach and they",
"described how it really made engineering",
"a lot easier because a lot of the hand",
"created features weren't necessary",
"anymore you still need some but it was",
"just simpler right so they ended up",
"something that was more accurate and but",
"perhaps even more importantly it",
"required less maintenance right so I",
"wouldn't say you know it's the only tool",
"that you need in your toolbox for",
"analyzing tabular data but you know",
"where else I used to use random forests",
"99% of the time when I was doing machine",
"learning with tabular data I knew or",
"Nets 90% of the time it's it's it's kind",
"of my standard first go-to approach now",
"and it tends to be pretty reliable",
"pretty effective one of the things",
"that's made it difficult is that until",
"now there hasn't been an easy way to",
"kind of create and train tabular neural",
"Nets like nobody's really made it",
"available on a library so we've actually",
"just created fast AI tabula and I think",
"this is pretty",
"the first time that's become really easy",
"to to use neural nets with tabular data",
"and so let me show you how easy it is",
"this is actually coming directly from",
"the examples folder in the FASTA guy",
"repo I haven't changed at all and as per",
"usual as well as importing first AI you",
"should import your application so in",
"this case it's tabular we assume that",
"your data is in a panda's data frame a",
"panda's data frame is kind of the",
"standard format for tabular data in",
"Python and it's lots of ways to get it",
"in there but probably the most common",
"might be PD don't read CSV but you know",
"whatever your data is in you can",
"probably get it into a panda's data",
"frame easily enough okay what are the",
"10% of cases where you would not default",
"to neural nets good question I guess I",
"still tend to kind of give them a try",
"but yeah I don't know it's it's it's",
"kind of like as you do things for a",
"while you start to get a sense of the",
"areas where things don't quite work as",
"well I have to think about that during",
"the week I don't think I have a rule of",
"thumb but I would say you may as well",
"try both like I would say try a random",
"forest and try on your own net they're",
"both pretty quick and easy to run and",
"see how it looks and if they're roughly",
"similar I might go and dig into H and",
"see if I can make them better and better",
"but you know if the random forest is",
"doing way better I'd probably just stick",
"with that use whatever works so I",
"currently have the wrong notebook in the",
"lesson repo so I'll update it after the",
"class so sorry about that so we start",
"with the data in a data frame and so",
"we've got a little thing adult sample",
"it's a it's a classic old data set I",
"have to dig up the citation for it so",
"I've got put it in this sum in this",
"notebook but it's a pretty small simple",
"old data set that's good for",
"experimenting with basically and it's",
"CSV file so you can read it into a data",
"frame with pandas read CSV PD don't read",
"CSV if your data is in a relational",
"database pandas can read from that if",
"it's in spark or Hadoop pandas can read",
"from that pandas can read from most",
"stuff that you can throw at it so that's",
"why we kind of use it as a default",
"starting point and as per usual you know",
"it's I think it's nice to use the data",
"block API and so in this case the list",
"that we're trying to create is a tabular",
"list and we're going to create it from a",
"data frame and so you can tell it what",
"the data frame is and what the path that",
"you're going to use to kind of save",
"models and intermediate steps is and",
"then you need to tell it what are your",
"categorical variables and what are your",
"continuous variables so we're going to",
"be learning a lot more about what that",
"means to the neural net next week but",
"for now the quick summary is this your",
"independent variables are the things",
"that you're using to make predictions",
"with right so things like education and",
"marital status and age and so forth some",
"of those variables like age are",
"basically numbers they could be any",
"number you know you could be thirteen",
"point three six years old or nineteen",
"point four years old or whatever where",
"else things like marital status options",
"that can be selected from a discrete",
"group married single divorce whatever",
"sometimes those options might be",
"a lot more like occupation there's a lot",
"of possible occupations and sometimes",
"they might be binary could be just true",
"or false but anything which you can",
"select the the answer from a small group",
"of possibilities is called a categorical",
"variable and so we're going to need to",
"use a different approach in the neural",
"net to modeling categorical variables to",
"what we use for continuous variables for",
"categorical variables we're going to be",
"using something called embeddings which",
"we'll be learning about later today for",
"continuous variables they could just be",
"sent into the neural net just like",
"pixels in a neural net can because like",
"pixels in a neural net are already",
"numbers these continuous things are",
"already numbers as well so that's that's",
"easy okay so that's why you have to tell",
"the tabular list from data frame which",
"ones are which there are some other ways",
"to do that by pre-processing them in",
"pandas to make things categorical",
"variables but it's kind of nice to have",
"one API for doing everything you don't",
"have to think too much about it then",
"we've got something which is a lot like",
"transforms in in computer vision",
"transforms in computer vision do things",
"like flip a photo when it's access or",
"turn it a bit or brighten it or",
"normalize it",
"but for tabular data instead of having",
"transforms we have things called",
"processes and they're nearly identical",
"but the key difference which is quite",
"important is that a processor is",
"something that happens ahead of time",
"right so we basically pre-process the",
"data frame rather than doing it as we go",
"right so transformations are really that",
"data augmentation where you want to like",
"randomize it and do it differently each",
"time where else processes the things",
"that you want to do once ahead of time",
"so we have a number of processes in the",
"Farseer library and the ones we're going",
"to use this time are fill missing so",
"that's going to look for missing values",
"and deal with them some way we're going",
"to find categorical variables and turn",
"them into pandas categories and we're",
"going to do normalization ahead of time",
"which is to take continuous variables",
"and subtract their mean and divided by",
"them by their standard deviation so",
"they're 0 1 variables the way we deal",
"with missing data we'll talk more about",
"next week but in short we replace it",
"with a median and add a new column which",
"is a binary column of saying whether",
"that was missing or not normalization",
"there's an important thing here which is",
"in fact for all of these things whatever",
"you do to the training set you need to",
"do exactly the same thing to the",
"validation set and the test set so",
"whatever you replaced your missing",
"values with you need to replace them",
"with exactly the same thing in the",
"validation set so first AI handles all",
"these details for you they're the kinds",
"of things that if you have to do it",
"manually at least if you like me you'll",
"screw it up",
"lots of times until you finally get it",
"right so that's what these processes",
"here then we're going to split into",
"training versus validation sets and in",
"this case we do it by providing a list",
"of indexes so the index is from 800 to",
"1,000 it's very common I don't quite",
"remember the details of this data set",
"but it's very common for wanting to keep",
"your validation sets to be contiguous",
"groups of things like if they're map",
"tiles there should be the map tiles that",
"are next to each other if their time",
"periods they should be time period you",
"know days that are next to each other if",
"their video frames there should be video",
"frames next to each other",
"because otherwise you're kind of",
"cheating right so it's often a good idea",
"to use split by DX and to grab a range",
"that's next to each other if your data",
"has some kind of structure like that or",
"find some other way to structure it in",
"that way all right so that's now given",
"us a training and a validation set we",
"now need to add labels and in this case",
"the labels can come straight from the",
"data frame we grabbed earlier so we just",
"have to tell it which column it",
"and so the dependent variable is I think",
"it's whether they're making over $50,000",
"salary that's the thing we're trying to",
"predict in this case we'll talk about",
"test sets later but in this case we can",
"have a test set and finally get our data",
"bunch so at that point we have something",
"that looks like this",
"okay so there is our there is our data",
"and then to use it it looks very",
"familiar you get a loner in this case",
"it's a tabular learner passing in the",
"data some information about your",
"architecture and some metrics Andrew",
"then call fit you have some questions",
"all right let's hit the questions how to",
"combine an LP tokenize data with",
"metadata such as tabular data with fast",
"AI for instance for imbd classification",
"how to use information like who the",
"actors are your maid Jean right it",
"cetera",
"yeah we're not quite up for that yet so",
"we need to learn a little bit more about",
"how euronet architecture as well work",
"and but conceptually it's kind of the",
"same as the way we combine categorical",
"variables and continuous variables",
"basically in the neural network you can",
"have two different sets of inputs",
"merging together into some layer could",
"go into an early layer or into a later",
"layer it kind of depends if it's like",
"text and an image and some metadata you",
"probably want the text going into an",
"errand in the image going into a CNN the",
"metadata going into some kind of tabular",
"model like this and then you'd have them",
"basically all concatenated together and",
"then go through some fully connected",
"layers train them into end will probably",
"largely get into that in part two in",
"fact we made entirely get in that into",
"part part two I'm not sure if we have",
"time to cover it in in part one but",
"conceptually it's it's a fairly simple",
"extension of what we'll be learning in",
"the next three weeks",
"next question is do you think things",
"like scikit-learn annex G boost will",
"eventually become outdated will everyone",
"use deep learning tools in the future",
"except for maybe small datasets I have",
"no idea I'm not good at making",
"predictions I I'm not a machine learning",
"model",
"I mean XJ boost is a really nice piece",
"of software there's quite a few really",
"nice pieces of software for gradient",
"boosting in particular they have some",
"really nice features or actually random",
"forests in particular has some really",
"nice features for interpretation which",
"I'm sure will find similar versions for",
"neural nets but they don't necessarily",
"exist yet so I don't know so now they're",
"both useful tools scikit-learn you know",
"is a library that's often used for kind",
"of pre-processing and running models",
"yeah I mean again it's it's hard to",
"predict where things will end up it's",
"it's kind of in some ways it's more",
"focused on some older approaches to",
"modeling but I don't know they keep on",
"adding new things so we'll see I keep",
"trying to incorporate more scikit-learn",
"stuff into fast AI and then I keep",
"finding ways I think I can do it better",
"and I throw it away again so so that's",
"why there's still no scikit-learn",
"dependencies in fast AI I keep finding",
"other ways to do self okay so we're",
"gonna learn what layers equals means",
"either towards the end of class today or",
"the start of class next week but this is",
"where we're basically defining our",
"architecture just like when we chose",
"ResNet 34 or whatever for convinence",
"will look at more about metrics in a",
"moment but just to remind you metrics",
"are just the things that get printed out",
"they don't change our model at all so in",
"this case we're saying I want you to",
"print out the accuracy to see how we're",
"doing",
"okay so that's how to do tabular this is",
"going to work really well because we're",
"going to hit our break soon and the idea",
"was that after three and a half lessons",
"we're going to hit the end of all of the",
"quick overview of applications and then",
"we're going to go down the other side I",
"think we're going to be to the minute",
"we're going to hit it right because the",
"next one is collaborative filtering okay",
"so collaborative filtering is where you",
"have information about who bought what",
"or who liked what you know it's",
"basically something where you have",
"something like a user or a reviewer or",
"whatever and information about what",
"they've bought or what they've written",
"about or what they reviewed right so in",
"the most basic version of collaborative",
"filtering you just have two columns",
"something like user ID and movie ID and",
"that just says this user bought that",
"movie this user bought that movie this",
"user for that review so for example",
"Amazon has a really big list of user IDs",
"and product IDs are like what did you",
"buy then you can add additional",
"information to that table such as oh",
"they left a review what review did they",
"give it so it's now like user ID movie",
"ID number of stars you could add a",
"timecode so like this user bought this",
"product at this time and gave it this",
"review but they're all basically the",
"same kind of structure so there's kind",
"of like two ways you could draw that",
"collaborative filtering structure one is",
"kind of a two-column approach where",
"you've got like user and I don't know",
"movie all right and you've got user ID",
"movie oh do you know each each pair",
"basically describes that user watch that",
"movie possibly also plus number of stars",
"you know three or one whatever well the",
"other way you could write it",
"would be you could have like all the",
"users down here and all the movies along",
"here right and and then you know you can",
"look and find a particular cell in there",
"to find out you know could be the rating",
"of that user for that movie or there's",
"just a one there if that user watch that",
"movie or whatever so there's like two",
"different ways of representing the same",
"information conceptually it's often",
"easier to think of it this way right but",
"most of the time you won't store it that",
"way explicitly because most of the time",
"you'll have what's called a very sparse",
"matrix which is to say most users",
"haven't watched most movies or most",
"customers haven't purchased most",
"products so if you store it as a matrix",
"where every combination of customer and",
"product is a separate cell in that",
"matrix it's going to be enormous so you",
"tend to store it like this or you can",
"store it as a matrix using some kind of",
"special sparse matrix format and if that",
"sounds interesting you should check out",
"Rachel's computational linear algebra",
"course on first AI where we have lots",
"and lots and lots of information about",
"sparse matrix storage approaches for now",
"though we're just going to kind of keep",
"it in this format on the left hand side",
"so for collaborative filtering there's a",
"really nice dataset called movie lens",
"created by the group lens group very",
"hopefully and you can download various",
"different sizes 20 million ratings a",
"hundred thousand ratings we've actually",
"created a an extra small version for",
"playing around with which is what we'll",
"start with today and then",
"probably next week we'll use the bigger",
"version but so you can grab the small",
"version using your RL ml sample and it's",
"a CSV so you can read it with pandas and",
"here it is right it's basically a list",
"of user IDs we don't actually know",
"anything about who these users are",
"there's some movie IDs there is some",
"information about what the movies are",
"but we won't look at that until next",
"week and then there's the rating and",
"then there's the timestamp",
"we're going to ignore the timestamp now",
"so that's a subset of our data that's",
"the head so the head in pandas is just",
"the first few rows so now that we've got",
"a data frame I mean the nice thing about",
"collaborative filtering is is it's it's",
"incredibly simple like that's all the",
"data that we need so you can now go",
"ahead and say get collaborative learner",
"and you can actually just pass in the",
"data frame directly the the architecture",
"you have to tell it how many factors you",
"want to use and we're going to learn",
"what that means after the break and then",
"something that can be helpful is to tell",
"it what the range of scores are and",
"we're going to see how that helps up the",
"brick as well now in this case the",
"minimum score is zero it's an excellent",
"score is five so now that you've got a",
"learner you can go ahead and call fit",
"one cycle and trains for a few a pox and",
"there it is so at the end of it you now",
"have something where you can pick a user",
"ID and a movie ID and guess whether or",
"not that user will like that movie",
"there's a lot of so this is obviously a",
"super useful application that a lot of",
"you are probably going to try over",
"during the week in past classes a lot of",
"people have taken this collaborative",
"filtering approach back to their",
"workplaces and and discovered that using",
"it in practice is much more tricky than",
"this because in practice you have",
"something called the cold start problem",
"so the cold start problem is that the",
"time you particularly want to be good at",
"recommending movies is when you have a",
"new user and the time you particularly",
"care about recommending a movie is when",
"it's a new movie but at that point you",
"don't have any data in your",
"collaborative filtering system and it's",
"really hard as I say this we don't",
"currently have anything built into fast",
"AI to handle the cold start problem and",
"that's really because the cold start",
"problem the only way I know of to solve",
"it in fact the anyway I think that",
"conceptually you can solve it is to have",
"a second model which is not a",
"collaborative filtering model but a",
"metadata driven model for new users or",
"new movies I don't know if Netflix still",
"does this but certainly what they used",
"to do when I signed up to Netflix was",
"they started showing me lots of movies",
"and saying have you seen this did you",
"like it have you seen this did you like",
"it you know and so they fixed the cold",
"start problem through the UX so there",
"was no you know called start problem",
"they found like 20 really common movies",
"and asked me if I liked them they used",
"my replies to those 20 to show me 20",
"more that I might have seen and you know",
"by the time I had gone through 60 I",
"wasn't you know there was no cold start",
"problem anymore and for new movies it's",
"not really a problem because like the",
"first hundred users who haven't seen the",
"movie you know go in and say whether",
"they liked it and then the next hundred",
"thousand the next million it's not a",
"cold start problem anymore",
"but the other thing you can do if you",
"for whatever reason kind of can't go",
"through that UX of like asking people",
"did you like those things so for example",
"if you're selling products and you don't",
"really want to show them like a big",
"selection of your products and say did",
"you like this because you just want them",
"to buy you could instead try and use a",
"metadata based kind of tabular model you",
"know what what geography did they come",
"from maybe you know their age and sex",
"you know you can try and make some",
"guesses about the initial",
"recommendations",
"that's so collaborative filtering is",
"specifically for once you have a bit of",
"information about your users and movies",
"or customers and products or or whatever",
"yeah",
"how does the language model trained in",
"this manner perform on code-switch data",
"such as Hindi written in English words",
"or text with a lot of emojis and then do",
"it the second question there certainly",
"yeah that's a good question so text with",
"emojis it'll be fine",
"there's not many emojis in Wikipedia and",
"where they are in Wikipedia it's more",
"like a Wikipedia page amount about the",
"emoji rather than the emoji being used",
"in you know sensible place but you can",
"insured through this language model fine",
"tuning where you take a corpus of text",
"where people are using emojis in usual",
"ways and so you fine-tune the wiki text",
"language model to your reddit or Twitter",
"or whatever language model and there",
"aren't that many emojis right if you",
"think about it there's like hundreds of",
"thousands of possible words that people",
"can be using but a small number of",
"possible emojis so it'll very quickly",
"learn how those emojis are being used so",
"that's that's a piece of cake",
"so I'm not very familiar with Hindi but",
"I'll take an example I'm very familiar",
"with which is Mandarin in Mandarin you",
"could have a model that's trained with",
"Chinese characters so there's kind of",
"five or six thousand Chinese characters",
"in common use but there's also a",
"romanization of those characters called",
"an opinion and it's a bit tricky because",
"although there's a nearly direct mapping",
"from the character to the pinyin I mean",
"there is a direct mapping the",
"pronunciations not exactly direct there",
"isn't a direct mapping from the pinyin",
"to the character because one pinyin is",
"was corresponds to multiple characters",
"so the first thing to note is that if",
"you're going to use this approach for",
"Chinese you would need to start with",
"his language model so actually first AI",
"has something called a model zoo where",
"we're adding more and more language",
"models for different languages and also",
"increasingly for different domain areas",
"like English medical texts or even",
"language models for things other than",
"NLP like genome sequences molecular data",
"musical midi notes and so forth so you",
"would obviously start there to then",
"convert that you know that'll be in you",
"know either simplified or traditional",
"chinese to then convert that into a if",
"you want to do pinyin you could either",
"kind of map the vocab directly or as",
"you'll learn these multi-layer models",
"it's only the first layer that basically",
"converts the the tokens into a set of",
"vectors you can actually throw that away",
"and fine tune just a the first layer of",
"the model so that second part is going",
"to require a bit more few more weeks of",
"learning before you exactly understand",
"how to do that and so forth but if",
"there's something you're interested in",
"doing we can talk about it on the forum",
"because it's a kind of a nice test of",
"understanding so what about time series",
"on tabular data is there an RNN model",
"involved in tabular models so we're",
"going to look at time series tabular",
"data next week and but the short answer",
"is generally speaking you don't use a",
"RNN for time series tabular data but",
"instead you extract a bunch of columns",
"for things like day of week is it a",
"weekend is it a holiday was the store",
"open stuff like that and it turns out",
"that adding those extra columns which",
"you can do somewhat automatically",
"basically gives you state-of-the-art",
"results there are some good uses of RN",
"ends for",
"for time series but not really for these",
"kind of tabular style time series like",
"you know retail store logistics",
"databases and stuff like that okay and",
"is there a source to learn more about",
"the cold start problem I'm gonna have to",
"look that up if you know a good resource",
"please pin turn it on the forums okay",
"okay so that is both the break in the",
"middle of lesson four it's the halfway",
"point of the course and it's the point",
"at which we have now seen an example of",
"all the key applications and so the rest",
"of this course is going to be digging",
"deeper into how they actually work",
"behind the scenes more of the theory",
"more of how the code the source code is",
"written and so forth so it's a good time",
"to have a nice break come back and",
"furthermore it's my birthday today so",
"it's really you know a special moment so",
"yeah so let's have a break and come back",
"at five past eight so Microsoft Excel",
"this is one of my favorite ways to",
"explore data and understand models now",
"make sure I put this in the repo and",
"actually this one we can probably",
"largely do in Google sheets I've tried",
"to move as much as I can over the last",
"few weeks into Google sheets but I just",
"keep finding it's just such a terrible",
"product so I yeah you know please try to",
"find a copy of Microsoft Excel because",
"there's nothing close I've tried",
"everything anyway spreadsheets get up",
"a bad rap from people that basically",
"don't know how to use them just like",
"people who better they'll spend their",
"lives on Excel and then they start using",
"Python and they're like what the hell is",
"this stupid thing I mean you know it",
"takes thousands of hours to get really",
"good at spreadsheets but a few dozen",
"hours to get competent at them and once",
"you're competent at them you can see",
"everything in front of you it's all laid",
"out it's it's really great I'll give you",
"one spreadsheet tip today which is if",
"you hold down the ctrl key or command",
"key on your keyboard and press the arrow",
"keys",
"here's control right it takes you to the",
"end of a block of a table that you're in",
"and like it's by far the best way to",
"move around the place so there you go so",
"in this case you know I want to like",
"skip around through this table so I can",
"hit ctrl down right to get to the bottom",
"right control left up to get to the top",
"left cuz I skip around and see what's",
"going on so here's hey wait so here's",
"some data and as we talked about one way",
"to look at collaborative filtering data",
"is like this and so what we did was we",
"grabbed from the movie lens data the",
"people that watched the most movies and",
"the movies that were the most watched",
"and just filtered the data set down to",
"those 15 and as you can see when you do",
"it that way it's not sparse anymore",
"there's just a small number of yeah",
"there's a small number of gaps right so",
"this is something that we can now build",
"a model with",
"and so how can we build a model like",
"what we want to do is we want to create",
"something which can predict for user",
"true 9-3 will they like movie 49 for",
"example okay",
"so we've got to come up with some way of",
"you know some function that can",
"represent that decision and so here's a",
"simple possible approach and so we're",
"going to take this idea of doing some",
"matrix multiplications so I've created",
"here a random matrix so here's one",
"matrix of random numbers and I've",
"created here and other matrix of random",
"numbers more specifically for each movie",
"I've created five random numbers and for",
"each user I've created five random",
"numbers and so we could say then that",
"user 14 movie 27 did they like it or not",
"well they're parading what we could do",
"would be to multiply together this",
"vector and that vector we can do a dot",
"product right and here's the dot product",
"right and so then we can basically do",
"that for every possible thing in here",
"we've got the dot product and you know",
"thanks to spreadsheets we can just do",
"that in one place and copy it over and",
"it fills in the whole thing for us why",
"would we do it this way well this is the",
"basic starting point of a neural net",
"isn't it a basic starting point of a",
"neural net is that you take the matrix",
"multiplication of two matrices and",
"that's that's what your first layer",
"always is and so we just have to come up",
"with some way of saying like well what",
"are two matrices that we can multiply",
"and",
"clearly you know you need a matrix for a",
"user you know or a vector for a user a",
"matrix for all the users and a vector",
"for a movie or a matrix for all the",
"movies and multiply them together and",
"you get some numbers right like so they",
"don't mean anything yet they're just",
"random right but we can now use gradient",
"descent to try to make these numbers and",
"these numbers give us results that are",
"closer to what we wanted so how do we do",
"that well we set this up now as a as a",
"linear model all right so the next thing",
"we need is a loss function so we can",
"calculate our loss function by saying",
"well okay movie 3 for user ID 14 should",
"have been a rating of 3 with this random",
"matrices it's actually a rating of 0.9 1",
"so we can find the sum of squared errors",
"would be 3 minus 0.9 1 squared and then",
"we can add them up so there's actually a",
"sum squared in excel already some X",
"minus y squared so we can use just some",
"X minus y squared function passing in",
"those two ranges and then divide by the",
"count to get the mean so here is a",
"number that is the mean that's exactly",
"the square root of the mean squared",
"error so like you sometimes you'll see",
"people talk about MSE so that's the mean",
"squared error sometimes you'll see our",
"MSE that's the root mean squared error",
"so since I've got a square root at the",
"front this is the",
"root mean square error so we have a loss",
"so now all we need to do is use gradient",
"descent to try to modify our weight",
"matrices to make that loss smaller so",
"Excel will do that for me I have other",
"them installed so it's probably worth",
"knowing how to do that",
"so we have to install add-ins not",
"solvers there okay this is obviously",
"forgotten where it was oh yeah okay",
"so the gradient descent solver in Excel",
"is called solver and it just does normal",
"gradient descent so you just go data",
"solver you need to make sure that in",
"your settings that you've enabled the",
"solver extension it comes with Excel and",
"all you need to do is say which cell",
"represents my loss function so there it",
"is c41 right so which where is your loss",
"function stored which cells contain your",
"your variables right since so you can",
"see here I've got H 90 into V 23 which",
"is up here and B 25 to have 39 which is",
"over there and then you can just say",
"okay set your loss function to a minimum",
"by changing those cells and solve and",
"you'll see the starts a 2.81 and you can",
"see the numbers going down and so all",
"that's doing is using gradient descent",
"exactly the same way that we did when we",
"did it manually in the notebook the",
"other day okay but it's it's rather than",
"solving the means grid error for a at B",
"in the a at X in the Python instead it",
"is solving the loss function here which",
"is the mean squared error if the dot",
"product of each of those vectors by each",
"of these vectors and so there it goes",
"so we'll let that run for a little while",
"and see what happens",
"but basically in in micro here is a",
"simple way of creating a neural network",
"which is really in this case it's like",
"just a single linear layer with gradient",
"descent to solve a collaborative",
"filtering problem so let's go back and",
"see what we do over here",
"so over here we used get collab learner",
"okay so the the function that was called",
"in the notebook was get collab learner",
"and so as you dig deeper into deep",
"learning one of the really good ways to",
"dig deeper into deep deep learning is to",
"dig into the fast AI source code and see",
"what's going on and so if you're going",
"to be able to do that you need to know",
"how to use your editor well enough to",
"dig through the source code right and",
"basically there's two main things you",
"need to know how to do one is to jump to",
"a particular symbol like a particular",
"class or function by like by its name",
"and the other is that when you're",
"looking at a particular symbol to be",
"able to jump to its its implementation",
"so for example in this case I want to",
"find Det collab Lana so in most in most",
"editors including the one I use VM you",
"can set it up so that you can kind of",
"hit tap or something and it jumps",
"through all the possible completions and",
"you can hit enter and it jumps and it",
"jumps straight to the definition for you",
"alright so here is the definition of get",
"collab alona and as you can see it's",
"pretty small as these things tend to be",
"and the keys in this case it kind of",
"wraps data frame and automatically",
"creates the data bunch for you because",
"it's so simple but the key thing it does",
"then is to create a model the whole",
"particular kind which is an embedding",
"bias model passing in the various things",
"you asked for so you want to find out in",
"your editor how you jump to the",
"definition of that which",
"in vim you just hit control right square",
"bracket and here is the definition of",
"embedding bias and so now we have",
"everything on screen at once and as you",
"can see there's not much going on in so",
"the models that are being created for",
"you by first AI are actually paid watch",
"models and a PI torch model is called an",
"NN module that's the name in hi torch of",
"their models it's a little more nuanced",
"than that but that's a good starting",
"point for now and when a PI torch and",
"end module is is run when you calculate",
"the value know the result of that layer",
"or neural net or whatever specifically",
"it always calls a method for you called",
"forward so it's in here that you get to",
"find out how this thing's actually",
"calculated when the model is built at",
"the start it calls this thing called",
"underscore underscore in it and the",
"supportt underscore and as I think we've",
"briefly mentioned before in - people",
"tend to call this dunder init double",
"underscore in it so dunder init is how",
"we create the model and forward is how",
"we run the model one thing if you're",
"watching carefully you might notice is",
"there's nothing here saying - how to",
"calculate the gradients of the model and",
"that's because height which does it for",
"us okay so you only have to tell it how",
"to calculate the output of your model",
"and PI torch will go ahead and calculate",
"the gradients for you",
"and so in this case the model contains a",
"set of weights through a user a set of",
"weights for an item a set of biases for",
"a user a set of biases for an item and",
"each one of those is called",
"is coming from this thing called get",
"embedding",
"so let's see get embedding so here is",
"the definition of get embedding and all",
"it does basically is it calls this",
"height watch thing called n n dot",
"embedding so in pi torch they have a lot",
"of like standard neural network layers",
"set up for you so gates and embedding",
"and then this thing here is it just",
"randomizes it so this is something which",
"creates normal random numbers through",
"the embedding so what's an embedding and",
"embedding not surprisingly is a matrix",
"of weights specifically it's a matrix of",
"weights specifically an embedding is a",
"matrix of weights that looks something",
"like this",
"it's a matrix of weights which you can",
"basically look up into and grab one item",
"out of it right so basically any kind of",
"weight matrix and we're going to be",
"digging into this in a lot more detail",
"in the coming lessons but an embedding",
"matrix is just a weight matrix that is",
"designed to be something that you kind",
"of index into it as an array and grab",
"one vector out of it all right that's",
"what an embedding matrix is and so in",
"our case we have an embedding matrix for",
"a user and an embedding matrix for a",
"movie and here we have been taking the",
"dot product of them all right but if you",
"think about it that's not quite enough",
"because we're missing this idea that",
"like maybe there are certain movies that",
"everybody likes more maybe there are",
"some users that just tend to like movies",
"more all right so I don't really just",
"want to multiply these two vectors",
"together but I really want to add a",
"single number of like how popular is",
"this movie and add a single number of",
"like how much does this user like",
"is in general so those are called bias",
"terms remember how I said like there's",
"this kind of idea of like bias and we",
"the way we dealt with that in our",
"gradient descent notebook was we added a",
"column of ones okay but what we tend to",
"do in practice is we actually explicitly",
"say I want to add a bias term so we",
"don't just want to have prediction",
"equals dot product of these two things",
"we want to say it's the dot product of",
"those two things plus a bias term for a",
"movie plus a bias term for user ID so",
"that's basically what happens we when we",
"set up the model we set up the embedding",
"matrix for the users and the embedding",
"matrix for the items and then we also",
"set up the bias vector for the users and",
"the bias vector for the items and then",
"when we calculate the model we literally",
"just multiply the two together just like",
"we did right we just take that that",
"product call it dot right and then we",
"add the bias and then putting aside the",
"min and Max score for a moment that's",
"what we return so you can see that our",
"model is literally doing what we did",
"here with the tweak that we're also",
"adding a bias right so it's it's an",
"incredibly simple linear model and for",
"for these kinds of collaborative",
"filtering problems this kind of simple",
"linear model actually tends to work",
"pretty well and then there's one tweak",
"that we do at the end which is that in",
"our case we said that there's a min",
"score of zero and a max score of five",
"and so here's something to point out",
"here's something to point out so if you",
"have you know a range so you'd like you",
"do that dot product and you add on the",
"two biases and that gives you you know",
"that can give you any possible number",
"along the number line from very negative",
"through to very positive numbers but we",
"know that we always want to end up with",
"a number between zero and five let's say",
"that's five and of course this is zero",
"so what if we match that number line",
"like so",
"to this function okay and so the shape",
"of that function is called a sigmoid",
"right and so it's gonna asymptote to",
"five and it's gonna asymptote to zero",
"and so that way whatever whatever number",
"comes out of our dot product and adding",
"the biases if we then stick it through",
"this function it's never going to be",
"higher than five and never going to be",
"smaller than zero now strictly speaking",
"that's not necessary right because our",
"parameters could learn a set of weights",
"that gives about the right number right",
"so why would we do this extra thing if",
"it's not necessary well the reason is we",
"wouldn't make his life as easy for our",
"model as possible",
"so if we actually set it up so it's",
"impossible for it to ever predict too",
"much or ever predict too little then it",
"can spend more of its weights predicting",
"the thing we care about which is",
"deciding who's going to like what movie",
"so this is an idea we're going to keep",
"coming back to when it comes to like",
"making neural networks work better is",
"it's about all these little decisions",
"that we make to basically make it easier",
"for the network to learn the right thing",
"so that's the last tweak here we",
"as we take the result of this dot",
"product plus biases we put it through a",
"sigmoid and so a sigmoid is just a",
"function it's basically 1 over 1 plus e",
"to the X the definition doesn't much",
"matter but it just has the shape that I",
"just mentioned and that goes between 0 &",
"1 and if you then multiply that by max",
"minus min plus min then that's going to",
"give you something that's between min",
"score and Max score so that means that",
"this tiny little neural network I mean",
"it's a push to call it a neural network",
"but it is it's a neural network with",
"with one weight matrix and no none when",
"the arrow DS so it's kind of the world's",
"most boring neural network with a",
"sigmoid at the end that's actually",
"because it does have a non-linearity the",
"sigmoid at the end is the non-linearity",
"just it only has one layer of weights",
"that actually turns out to give close to",
"state-of-the-art performance like I've",
"looked up online to find out like what",
"are the best results people have on this",
"movie lends 100k database and the",
"results I get from this little thing",
"better than any of the results I can",
"find from like the standard commercial",
"products that you can download that are",
"specialized for this and the trick seems",
"to be that adding this little sigmoid",
"makes a big difference and did you have",
"a question there was a question",
"you set up your VIN and I've already",
"linked here dot them RC but I wanted to",
"know if you had more to say about that",
"they really like your setup you like my",
"setup there's almost nothing in my setup",
"it's pretty bare honestly yeah I I mean",
"whatever you're doing with your editor",
"you probably want it to look like this",
"which is like when you've got a class",
"that you're not currently working on it",
"should be this is called folded this is",
"what folding right it should be closed",
"up so you can't see it and so you",
"basically want something where it's easy",
"to close and open fold so them already",
"does all this for you and then as I",
"mentioned you also want something where",
"you can kind of jump to the definition",
"of things which in VM it's called using",
"tags so for the jump to the definition",
"of learner basically them already does",
"all this for you you just have two",
"instructions that my VMRC is minimal I",
"basically hardly use any extensions or",
"anything",
"another great editor who uses a vs code",
"Visual Studio code it's free and it's",
"it's awesome and it has all the same",
"features that you're seeing that vim",
"does basically the s code does all of",
"those things as well I quite like using",
"vim because I can use it on the remote",
"machine and play around but you can of",
"course just clone get onto your the git",
"repo into your local computer and open",
"it up and vs code to play around with it",
"just don't like don't try and look",
"through the code just on github or",
"something like that's going to drive you",
"crazy you need to be able to open it and",
"close it and jump and jump back maybe",
"people can create some threads on the",
"forum for them tips fierce code tips",
"sublime tips whatever yeah for me I",
"would say like if you're gonna pick an",
"editor if you want to use something on",
"your local I would go with vs code today",
"I think it's the best if you want to use",
"something on the terminal side I would",
"go with them or Emacs to me there",
"they're clear winners",
"so what I wanted to close with today is",
"to kind of take this collaborative",
"filtering example and describe how we're",
"going to build on top of it for the next",
"three lessons to create the more complex",
"neural networks we've been seeing and so",
"roughly speaking you know this is the",
"bunch of concepts that we need to learn",
"about let's think about let's think",
"about",
"what happens when when you're using a",
"CNN - or you know whatever a neural",
"network to do image recognition",
"basically let's take a single pixel",
"right you've got lots of pixels but",
"let's take a single pixel so you've got",
"a red a green and a blue pixel okay and",
"so each one of those is some number",
"between Norton 255 well we kind of",
"normalized them so they're you know",
"floating point between well with the",
"mean of 0 and a standard deviation of 1",
"but let's just do that you know let's",
"say whatever they're like do that not to",
"255 fish and so it's like 10 20 30",
"whatever okay so what do we do with",
"these well what we do is we take we",
"basically treat that as a vector and we",
"multiply it by a matrix right so this",
"matrix depending on how you think of the",
"rows and the columns let's trade the",
"matrix is having three rows and then how",
"many columns well you get to pick right",
"you get to pick just like with the",
"collaborative filtering version I",
"decided to pick a vector of size five",
"for each of my embedding vectors right",
"so that would mean that that that's an",
"embedding basically of size five right",
"you can get to pick how big your weight",
"matrix is so let's make it size five",
"okay so this is three five five so",
"initially this weight matrix contains",
"random numbers remember when we looked",
"up get embedding weight matrix just now",
"and there were like two lines the first",
"line is like create the matrix and the",
"second was fill it with random numbers",
"that's what we do",
"right I mean lore gets hidden behind the",
"scenes by fast AI and PI torch that's",
"all it's doing just creating a matrix of",
"random numbers when you set it up and",
"the number of rows has to be three to",
"match the",
"and the number of columns can be as big",
"as you like and so after you multiply",
"the vector the input vector by that",
"weight matrix you're going to end up",
"with a vector of size five so people",
"often asked like how much linear algebra",
"do I need to know to be able to do deep",
"learning this is the amount you need",
"right so and and if you're not familiar",
"with this that's that's fine you need to",
"know about matrix products now you don't",
"need to know a lot about them you just",
"need to know like math and like",
"computationally what are they what do",
"they do and you've got to be very",
"comfortable with like if a you know a",
"matrix of size bla times a matrix of",
"size bla gives a matrix of size bla like",
"powder the dimensions match up so if you",
"have three and they remember in num PI",
"and PI torch we use at times three by",
"five gives a vector of size five okay",
"and then what happens next it goes",
"through an activation function such as r",
"lu which is just max zero comma X and",
"spits out a new vector which is of",
"course going to be exactly the same size",
"because no activation function changes",
"the size that it only changes the",
"contents so that's still a size five",
"what happens next",
"we multiply by",
"another matrix and again it can be any",
"number of columns but the number of rows",
"has to map nicely so it's going to be",
"five by whatever but oh so maybe this",
"one has you know five say by ten and so",
"that's going to give some output which",
"will be size ten and again we put that",
"through value and again that gives us",
"something of the same size okay and then",
"we can put that through another H matrix",
"actually just to make this a bit clearer",
"you'll see why in a moment I'm going to",
"use eight not ten just so that these",
"let's say we're doing digit recognition",
"right so there are ten possible digits",
"so my last weight matrix has to be ten",
"in size because then my that's going to",
"mean my final output is a vector of ten",
"in size and remember if you're doing",
"that digit recognition what happens did",
"we take our actuals right which is ten",
"in size and like if the number that",
"we're trying to predict was the number",
"three that's our like that's the thing",
"we're trying to predict then that means",
"that there is a three zero zero zero in",
"the third position right so what happens",
"is our neural net runs along okay it's",
"starting with our input and going wait",
"mate",
"Rick's value white bag tricks value",
"weight matrix final output and then we",
"compare these two together to see how",
"close they are how Plus they match using",
"some loss function and we'll learn about",
"all the loss functions that we use next",
"week for now the only one we've learned",
"his means great error and yeah we",
"compare the actual you can think of them",
"as probabilities for each of the 10 to",
"the actual each of the 10 to get a loss",
"and then we find the gradients of every",
"one as the weight matrices with respect",
"to that and we update the weight",
"matrices so the main thing I wanted to",
"show right now is the terminology we use",
"because it's really important these",
"things contain numbers specifically they",
"initially matrices containing random",
"numbers and we can refer to these yellow",
"things as in pi touch they're called",
"parameters",
"sometimes we'll refer to them as weights",
"although weights is slightly less",
"accurate because they can also be biases",
"right but you know we kind of use the",
"terms a little bit interchangeably but",
"strictly speaking we should call them",
"parameters and then after each of those",
"metrics products that calculates a",
"vector of numbers so here are some",
"numbers that are calculated by this one",
"here are some numbers that are",
"calculated by a weight matrix multiply",
"and then there are some other sets of",
"numbers that are calculated as a result",
"of a rail you as well as an activation",
"function okay either one",
"is called Accord activations so",
"activations and parameters both refer to",
"numbers right they are numbers the",
"parameters are numbers that are stored",
"they're used to make a calculation",
"activations are the result of a",
"calculation the numbers that are",
"calculated right so they're the two key",
"things you need to remember so use these",
"terms right and use them correctly and",
"accurately right and if you read these",
"terms they mean these very specific",
"things so don't mix them up in your head",
"and remember they're nothing weird and",
"magical they're very simple things an",
"activation is the result of either a",
"matrix multiply or an activation",
"function okay and a parameter are the",
"numbers inside the weights inside the",
"matrices that we multiply by okay that's",
"it and then there are some special",
"layers so every one of these things that",
"does a calculation all of these things",
"that does a calculation are all called",
"layers they're the layers of our neural",
"net so every layer results in a set of",
"activations because there's a",
"calculation that results in a set of",
"results okay",
"there's a special layer at the start",
"which is called the input layer and then",
"at the end you just have a set of",
"activations okay and we can refer to",
"those special I mean they're not special",
"mathematically but they're semantically",
"special we can call those the outputs",
"right so the important point to realize",
"here is the outputs of a neural net and",
"not actually like mathematically special",
"they're just the activations of a layer",
"and so what we did in our collaborative",
"filtering example we did something",
"interesting we actually added an",
"additional activation function right at",
"the very end",
"right we added an extra activation",
"function which was sigmoid and",
"specifically it was a scaled sigmoid",
"curve between 0 & 5 right and that's",
"really common right it's very common to",
"have an activation function as your last",
"layer and it's almost never going to be",
"a rel you because it's very unlikely",
"that what you actually want is something",
"that stops the truncates at zero it's",
"very often going to be a sigmoid or",
"something similar because it's very",
"likely that actually what you want is",
"something that's between two values okay",
"and kind of scaled in that way so that's",
"nearly it right so we've got inputs",
"weights activations activation functions",
"which we sometimes call nonlinearities",
"output and then the function that",
"compares those two things together right",
"is called the loss function which so far",
"we've used MSE yeah okay",
"and that's if that's enough for today so",
"what we're going to do what we're going",
"to do next week is we're going to kind",
"of add in a few more extra bits in which",
"is we're going to learn the loss",
"function that's used for classification",
"which is called cross-entropy",
"we're going to use the activation",
"function that's used for single label",
"classification which is called softmax",
"and we're also going to learn exactly",
"what happens when we do fine tuning in",
"terms of how these layers actually what",
"happens with unfreeze and what happens",
"when we create transfer learning so",
"thanks everybody",
"looking forward to seeing you next week"
}